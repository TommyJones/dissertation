# Linking Zipf's law to the LDA-DGP for Simulation Studies {#ldadgp}

LDA is a latent variable model and, as a result, it can be challenging to study. No ground truth exists against which to compare models and methods. Selecting hyper parameters when building models is notoriously difficult as little formal guidance exists to guide a researcher's choices. Lack of ground truth complicates development of new metrics and methods. How is one to know if a new method is an improvement or a new metric insightful?

Fortuately, LDA models a generative process.This allows researchers to generate synthetic data sets by sampling from the LDA data generating process (LDA-DGP) where they have chosen $K$, $\boldsymbol\alpha$, and $\boldsymbol\eta$ themselves. They then may use these simulated data sets to compare a model against a synthetic ground truth. Such "simulation studies" have a lengthy history in the statistical literature, going back to 1975 or further [@Hoaglin1975]. Yet to be valid, synthetic data from the LDA-DGP must conform to empirical statistical laws of language such as Zipf's and Heaps's laws. 

In this chapter I examine the LDA-DGP analytically and empirically to do the following:

1. Link Zipf's law---and by proxy Heaps's law---to the LDA-DGP,
2. Describe the effects hyper parameters of the LDA-DGP have in generating 4,096 synthetic corpora consistent with Zipf's and Heaps's laws, and
3. Describe the effects of misspecification of hyper parameters on the posterior of LDA models fit on a sample of the above synthetic data sets.

I find that setting $\boldsymbol\eta$ proportional to a power law produces synthetic data sets generated by the LDA-DGP that are consistent with Zipf's and Heaps's laws, that $\boldsymbol\eta$ is more important than previously appreciated, that $\boldsymbol\alpha$ may be less important than previously thought, and that $K$ has the biggest effects on the posterior when misspecified.

## Related Work

Work related to studying the LDA-DGP pulls from two seemingly disparate fields: topic modeling and complex systems theory. Complex systems theory deals in part with the emergence of power law distributions---as exemplified by some empirical laws of language---from complex systems [@cioffi2008power]. 

### Simulation Studies for LDA

Synthetic corpora appear commonly in topic modeling research. Shi et al. [@shi2019eval] list 14 works using synthetic corpora to study topic models. (See Table S3 in [@shi2019eval].) Most use some flavor of the LDA-DGP and focus on only one or two aspects of comparison between model and ground truth in the synthetic data set. Boyd-Graber, Hu, and Mimno suggest the use of semi-synthetic data---i.e., drawing simulations from the posterior of LDA models fit on real-world data---to capture properties of natural language [@boydgraber2017applications]. I am unaware of any work linking empirical laws of language to the LDA-DGP. Shi et al. link their simulation method to Zipf's law but it does not use the LDA-DGP to produce its simulations [@shi2019eval; @shi2019thesis]. 

Shi et al. go further than others and argue that use of synthetic corpora is "a principled approach" to evaluating topic models [@shi2019eval; @shi2019thesis]. Their approach does reproduce Zipf's law of language. Yet it does not use the LDA-DGP. It may represent a principled approach to studying probabilistic topic models with simulated data, but it is a parallel path to the one in this chapter. Using the LDA-DGP specifically allows for stronger statements related to pathological misspecification of LDA models. e.g., "Under the true model, then I would expect to see outcome A. Instead I see outcome B. Therefore, my model must be misspecified."[^linearanalogue] 

[^linearanalogue]: An analogue from linear regression might be, "under the true model, residuals are independent and identically distributed (_i.i.d._) Gaussian with mean zero. The residuals of my model are not _i.i.d._ Gaussian with mean zero. Therefore, my model must be misspecified."

### Empirical Language Laws

Altmann and Gerlach describe 9 universal laws purported to describe statistical regularities in human language [@altmann2015laws]. These laws are Zipf's [@zipf1949], Heaps's [@egghe2007untangling], Taylor's [@gerlach2014], Menzerath-Altmann [@altmann1980prolegomena], Recurrence [@zipf1949], Long-range correlation [@damerau1973tests], Entropy scaling [@altmann2015laws], Information content [@zipf1949], and various network topology laws [@altmann2015laws]. 

Of these laws, Zipf's and Heaps's laws are the most well known and relevant to LDA. The laws of Menzerath-Altmann, Recurrence, Long-range correlation, and Information content refer to properties of sub-words (e.g. length to information content), order of words, or proximity between words. LDA-DGP does not purport to model any of these. The law of Entropy scaling links entropy---in the information theoretic sense [@shannon2001mathematical]---to the number of words in a block of text. The LDA-DGP may or may not be able to reproduce this law. Similarly, some network views of a corpus may or may not apply to the LDA-DGP. Both may warrant future exploration but are less directly macroscopic properties of a corpus. 

Taylor's law may be relevant to the LDA-DGP but I leave its examination to future work as Zipf's and Heaps's laws are most commonly known as linguistic laws. Taylor's law was originally posed in the context of ecology [@taylor1961aggregation]. Its linguistic interpretation is that the standard deviation of the total number of words is proportional to the power of the mean of the total number of words in a corpus. Gerlach and Altmann explore the relationships between Zipf's, Heaps's, and Taylor's laws in the context of topic modeling [@gerlach2014]. Instead of using the LDA-DGP directly, they examine its asymptotic form, which is a Poisson process. 

#### Zipf's law
Zipf's law states that the frequency of a word is inversely proportional to the power of its frequency-rank. Zipf's law is not unique to any language as it appears to apply to all of them [@cancho2003]. Zipf's law has also been applied to the sizes of cities [@arshad2018zipf], casualties in armed conflict [@gillespie2015], and more. Zipf's law is a statement of a word's frequency and its rank. Yet if word frequencies in a corpus are plotted as a histogram, the power law relationship holds [@sole2008].

Empirical distributions of Zipf's law for large corpora demonstrate a relationship somewhat inconsistent with that predicted by Zipf's law. Some have proposed that the frequency-to-rank relationship is actually a set of broken power laws with one parametarization for the head of the distribution, another for the body, and a third parametarization for the tail. Yet, Ha et al. find that this is a trick of tokenization. "Language is not made of individual words but also consists of phrases of 2, 3 and more words, usually called n-grams for n=2, 3, etc." When including n-grams in both English and Chinese corpora, Ha et al. find that Zipf's law holds through the tail [@ha2002zipf]. Mandelbrot developed a generalization of Zipf's law that accounts for behavior at the head of the distribution [@mandelbrot1965information].

Formally, Zipf's law is

\begin{align}
F(r) \propto r^{-\gamma} \text{ for } \gamma \geq 1, r > 1
\end{align}

where $r$ is a word's rank, $F(r)$ is the frequency of a word's rank, and $\gamma$ is a parameter to be estimated. For human language $\gamma \approx 1$ [@cancho2003] and can be found through maximum likelihood estimation [@gillespie2015]. Altmann and Gerlach find $1.03 \leq \gamma \leq 1.58$ depending on the corpus and estimation method [@altmann2015laws]

Goldwater et al. explore the relationship between Zipf's law and then standard statistical models of language [@goldwater2011zipf]. They develop a framework for producing power law word frequencies in two stages. Critically, they link this framework to several models closely related to LDA but do not extend it to the LDA-DGP itself.

#### Heaps's law
Heaps's law states that the number of unique words in a corpus, $V$, scales sub-linearly with the total number of words in a corpus, $N$ [@heaps1978information]. Under mild assumptions, Heaps's law is asymptotically equivalent to Zipf's law [@kornai1999zipf]. Unlike Zipf's law, it is an increasing power law.

\begin{align}
V \propto N^\delta \text{ for } N > 1, 0 < \delta < 1
\end{align}

There are several ways to compute Heaps's law for a single corpus. Altmann and Gerlach use two methods: the first computes $V$ and $N$ for each document in the corpus and the second progresses over each word in a corpus calculating new values of $V$ and $N$ as each word is added. The latter approach works for single documents of sufficient length as well, such as a book [@altmann2015laws]. 

## The LDA Data Generating Process (LDA-DGP)

Assuming there are $D$ contexts, $K$ topics, $V$ unique words, $N$ total words and $N_d$ words in the $d$-th context, the LDA-DGP is as follows. For each word, $n$, in context $d$: 

1. Generate $\boldsymbol{B}$ by sampling $K$ topics $\boldsymbol\beta_k \sim \text{Dirichlet}(\boldsymbol\eta), \forall k \in \{1,2,...,K\}$
2. Generate $\boldsymbol\Theta$ by sampling $D$ documents $\boldsymbol\theta_d \sim \text{Dirichlet}(\boldsymbol\alpha), \forall d \in \{1,2, ..., D\}$
3. Then for each context, $d$
    1. Draw topic $z_{d,n}$ from $\text{Multinomial}(\boldsymbol\theta_d)$
    2. Draw word $w_{d,n}$ from $\text{Multinomial}(\boldsymbol\beta_{z_{d,n}})$
4. Repeat 1. and 2. $N_d$ times.

Note that context $d$ has $N_d$ words $\forall d \in \{1, 2, ..., D\}$ and that the total number of words in the corpus is $N = \sum_{d=1}^D N_d$.

## Linking the LDA-DGP to Zipf's law
Zipf's law describes the relative frequencies between words in a corpus. Therefore, the expected term frequency distribution of a corpus drawn from the LDA-DGP should describe the relationship between Zipf's law and the LDA-DGP. The following relationship is derived in Appendix 3:

\begin{align}
\mathbb{E}(w_v) &=
N \frac{\eta_v}{\sum_{v = 1}^V \eta_v} \label{eq:zipf}
\end{align}

where $w_v$ is the total number of times word $v$ was sampled across all $D$ contexts.

Equation \@ref(eq:zipf) implies that the relative frequencies of words $i$ and $j$ are given by their ranks in a power law relationship. In other words, the shape of $\boldsymbol\eta$ is a power law, given by Zipf's law. No elements of $\boldsymbol\alpha$ appear in \@ref(eq:zipf). As shown in Appendix 3, the $\alpha_i$'s sum and factor out of the derivation. This makes some intuitive sense. An author can theoretically write an entire corpus of documents about only one topic or an author may write a corpus of documents with an even distribution of topics. In either case the aggregate word frequencies must follow Zipf's law.

This result is consistent with the framework laid out by Goldwater et al. [@goldwater2011zipf]. The Dirichlet multinomial model - of which LDA is an application - is a special case of their framework. In this framework, the Dirichlet prior has a parameter following a power law. They did not put this in the specific form of LDA, however, as is done in this paper. Using this specification of the Dirichlet multinomial it is now possible to generate corpora with the statistical properties of natural language within an LDA framework. This facilitates exploring the properties of LDA under different conditions and the development of metrics aiding in model specification and post-estimation diagnostics. 

The relationship in \@ref(eq:zipf) also implies that one can estimate the shape of $\boldsymbol\eta$ from a document term matrix. Formally linking Zipf's law with the LDA-DGP we have

\begin{align}
\mathbb{E}(w_v) &=
N \frac{\eta_v}{\sum_{v = 1} ^ V \eta_v} =
C \cdot r_v ^{-\gamma}
\end{align}

$C$ and $\gamma$ may be estimated from data and $r_v$, the rank of word $v$, is known. Some algebra then yields

\begin{align}
\frac{\eta_v}{\sum_{v = 1}^V \eta_v} = \frac{C}{N}\cdot r_v ^ {-\gamma}, \forall v
\end{align}

What still remains is for a researcher to specify the magnitude of $\boldsymbol\eta$, in other words $\sum_{v=1}^V \eta_v$, and of course $\boldsymbol\alpha$ and the number of topics, $K$. The empirical evidence for specifying $\boldsymbol\eta$ as a power law rather than a flat prior is explored, though not resoved, in section 3.4.3.

```{r compare-hypers, fig.cap = "Frequency vs. rank curves plotted in log-log space for several synthetic corpora and one real corpus. The leftmost chart plots both a power law and symmetric $\\boldsymbol\\alpha$ with a symmetric $\\boldsymbol\\eta$. The center chart plots both a power law and symmetric $\\boldsymbol\\alpha$ with a power law $\\boldsymbol\\eta$. The rightmost chart plots real data from the NIH corpus against a power law $\\boldsymbol\\alpha$ and power law $\\boldsymbol\\eta$. Only when $\\boldsymbol\\eta$ is proportional to a power law do we see Zipf's law in synthetic corpora.", fig.width = 7, fig.height = 2.5}

library(tidyverse)
library(patchwork)

nih_plotmat <- read_rds("data-derived/zipf-analysis/nih-plotmat.rds")

p1 <- nih_plotmat |> ggplot() + 
  geom_line(aes(x = rank, y = pl_pl), color = NA, lty = 4, lwd = 1.25) + 
  geom_line(aes(x = rank, y = npl_npl), color = "#b2df8a", lty = 2, lwd = 1.25) +
  geom_line(aes(x = rank, y = pl_npl), color = "#a6cee3", lty = 4, lwd = 1.25) + 
  ylim(1, 10000) +
  scale_x_continuous(trans='log10') +
  scale_y_continuous(trans='log10') +
  theme(axis.text.x = element_text(angle=45, hjust=1)) +
  ylab("Frequency") +
  xlab("Rank") +
  ggtitle("Symmetric Eta")

p2 <- nih_plotmat |> ggplot() + 
  geom_line(aes(x = rank, y = pl_pl), color = "#1f78b4", lty = 4, lwd = 1.25) + 
  geom_line(aes(x = rank, y = npl_pl), color = "#33a02c", lty = 2, lwd = 1.25) + 
  ylim(1, 10000) + 
  scale_x_continuous(trans='log10') +
  scale_y_continuous(trans='log10') +
  theme(axis.text.x = element_text(angle=45, hjust=1)) +
  ylab("") +
  xlab("Rank") + 
  ggtitle("Power Law Eta")

p3 <- nih_plotmat |> ggplot() + 
  geom_line(aes(x = rank, y = nih), color = "#d95f02", lwd = 1.25) + 
  geom_line(aes(x = rank, y = pl_pl), color = "#1b9e77", lty = 4, lwd = 1.25) + 
  ylim(1, 10000) +
  scale_x_continuous(trans='log10') +
  scale_y_continuous(trans='log10') +
  theme(axis.text.x = element_text(angle=45, hjust=1)) +
  ylab("") +
  xlab("Rank") + 
  ggtitle("NIH Corpus")


plot(p1 + p2 + p3, common.legend = TRUE, legend="bottom")

```

Figure \@ref(fig:compare-hypers) plots word frequencies of simulated and real data on a log-log scale. The leftmost image plots word frequencies of simulated data sampled from the LDA-DGP with a symmetric $\boldsymbol\eta$. The light blue dashed and dotted line has a power law $\boldsymbol\alpha$ and the light green dashed line has a symmetric $\boldsymbol\alpha$. The center image plots word frequencies of simulated data sampled from the LDA-DGP with a power law $\boldsymbol\eta$. The green dashed and dotted line has a power law $\boldsymbol\alpha$ and the blue dashed line has a symmetric $\boldsymbol\alpha$. The rightmost image contains word frequencies from a random sample of 1,000 abstracts from grants awarded by the US National Institutes of Health (NIH) in 2014 [@nih], which is the solid orange line. Consistent with [@ha2002zipf], unigrams and bigrams are included. Crucially, stopwords have _not_ been removed from the NIH data, allowing the head of the distribution to reach its full magnitude. (Implications of this choice are considered in Section 3.5 below.) The dashed and dotted green line is the same as in the center image, provided as a comparison. All six plots have the same number of contexts and words. The left and center images confirm the implications of the derrivation in Appendix 3. Specifically, that to generate word frequencies consistent with Zipf's law, $\boldsymbol\eta$ must have a power law distribution regardless of the shape of $\boldsymbol\alpha$. The rightmost image demonstrates that simulated data can have a power law word frequency distribution similar to real world data.

Finally, the asymptotic equivalence between Zipf's and Heap's law indicates that $\boldsymbol\eta$ drives both Zipf's and Heap's laws, the two most commonly-understood emprical language laws.

## Experiments
Simulation studies allow researchers to know the true parameters that generated a data set and evaluate how an estimated model performs against "the truth". But if simulated data cannot replicate the key statistical properties of its real world counterpart, it calls into question any conclusion made with such synthetic data. However, linking the LDA-DGP Zipf's law (and Heaps's law) adds rigor to simulation studies of LDA by giving synthetic data the gross statistical properties of human language. 

In this section, I generate 4,096 synthetic data sets using different parametarizations of the LDA-DGP and baseline corpus parameters, such as number of contexts, vocabulary size, etc. Each of these corpora are consistent with Zipf's/Heaps's laws. After briefly describing the properties of these synthetic corpora, I use them to explore the effects of misspecifying hyper parameters when fitting LDA models.

### Synthetic Data Generation
Each data set is drawn from a model with a unique combination of the following hyper parameters, resulting in 4,096 unique combinations:

1. $\sum_{v=1} ^ V \eta_v \in \{50, 250, 500, 1000\}$
2. $\sum_{k = 1} ^ K \alpha_k \in \{0.5, 1, 3, 5\}$
3. $N_d \sim \text{Pois}(\lambda)$ where $\lambda \in \{50, 100, 200, 400\}$
4. $V \in \{1000, 5000, 10000, 20000\}$
5. $D \in \{500, 1000, 2000, 4000\}$
6. $K \in \{25, 50, 100, 200\}$

I choose $\boldsymbol\eta$ to be proportional to a power law, consistent with Appendix 3 and Zipf's law [@zipf1949]. I chose $\boldsymbol\alpha$ to be symmetric for all data sets. The magnitudes of the Dirichlet hyper parameters, $\boldsymbol\eta$ and $\boldsymbol\alpha$ vary while keeping their shapes fixed. The magnitude of the parameter of a Dirichlet distribution plays a strong role in tuning the covariance between draws from that distribution. A smaller magnitude means larger variability between draws. In other words, if $\left(\sum_{v=1}^V \eta_v \right)$ is smaller, topics are less linguistically similar. If $\left(\sum_{v=1}^V \eta_v \right)$ is larger, topics are more linguistically similar. Similarly, $\left(\sum_{k=1}^K \alpha_k \right)$ tunes the topical similarity of documents.

### Describing Properties of Synthetic Corpora

For each synthetic data set I calculate parameter values for Zipf's law using linear regression in log-log space. Specifically, I fit $\log_{10} w_v = \log_{10}C -\gamma\log_{10}r_v + \epsilon_v$, where $\epsilon_v$ is the linear regression error term for the $v$-th word. This method is often biased by low rank words. To compensate, I use the `poweRLaw` package [@rpowerlawjss] to select a threshold for excluding low rank words. 

I also calculate several statistics from the synthetic data that a researcher would have available to them. These are $N$, the total number of word instances and $\hat{V}$, the total *observed* vocabulary size. 

#### Results
Each of the synthetic corpora was generated with $\boldsymbol\eta$ such that its Zipf coefficient was the same, $\gamma = 1.07$. Even so, there was considerable variation in the observed Zipf coefficient, Zipf constant ($C$), and minimum word frequency used to estimate the two Zipf parameters, $gamma$ and $C$. I find that the estimate for $\gamma$ is biased downwards in most of the synthetic data sets. The estimated value of $\gamma$ is highly correlated with the number of topics and, to a lesser degree, the magnitude of $\boldsymbol\eta$ of the generating distribution. The Zipf constant, $C$, is largely correlated with the total volume of words in the data set. The minimum word frequency used to estimate the Zipf parameters is highly correlated with the number of topics in the generating distribution. The magnitude of $\boldsymbol\alpha$ does not appear to be a significant driver of any Zipf-related variable.

Heaps's law describes the relationship between the total number of words in a corpus and the size of the vocabulary. It is an increasing power law with a slope that gets flatter as the total number of words increases. In the context of a simulated corpus, it is related to the difference between the total size of the lexicon available to the DGP, $V$, and the total number of unique words observed. If every word is sampled at least once, then the number of observed words equals $V$. However, under Zipf's law it is more likely that the total number of observed words will be less than $V$. I find that having more topics is highly correlated with observing more words.

Figure \@ref(fig:dtm-hist) below plots histograms of the Zipf coefficient, $\gamma$, Zipf constant, $C$, minimum word frequency used to estimate the Zipf parameters, and the difference between the total vocabulary size and observed vocabulary size. The leftmost histogram for $\gamma$ has a dashed line at $\gamma = 1.07$, which was used to seed each of the simulations. The Zipf constant and minimum used word frequency are presented on a log scale. All four demonstrate variability and the downward bias of the estimated Zipf coefficient over the seeded value is apparent.


```{r dtm-hist, fig.cap = "Histograms of the Zipf coefficient, $\\gamma$, Zipf constant, $C$, minimum word frequency used to estimate the Zipf parameters, and the difference between the total vocabulary size and observed vocabulary size. The Zipf constant and minimum used word frequency are presented on a log scale.", fig.width = 7, fig.height = 2.5}

dtm_metrics <- read_rds("data-derived/zipf-analysis/dtm-metrics.rds")

h1 <- dtm_metrics |>
  ggplot() + 
  geom_histogram(aes(x = -1 * zipf2_coef), fill = rgb(0.221, 0.421, 0.358, 0.5)) +
  geom_vline(xintercept = 1.07, lty = 2, lwd = 1.25) +
  ylab("") + xlab("") +
  ggtitle("Zipf Coefficient") +
  theme(axis.text.x = element_text(angle=45, hjust=1)) +
  theme(axis.text.y = element_blank())

h2 <- dtm_metrics |>
  ggplot() + 
  geom_histogram(aes(x = zipf2_const ^ 10), fill = rgb(0.513, 0.287, 0.200, 0.5)) +
  ylab("") + xlab("") +
  ggtitle("Zipf Constant") +
  theme(axis.text.x = element_text(angle=45, hjust=1)) +
  theme(axis.text.y = element_blank()) +
  scale_x_continuous(trans = "log10")

h3 <- dtm_metrics |>
  ggplot() + 
  geom_histogram(aes(x = zipf_xmin), fill = rgb(0.280,0.317,0.403, 0.5)) +
  ylab("") + xlab("") +
  ggtitle("Min. Word Freq.") +
  theme(axis.text.x = element_text(angle=45, hjust=1)) +
  theme(axis.text.y = element_blank()) +
  scale_x_continuous(trans = "log10")

h4 <- dtm_metrics |>
  ggplot() + 
  geom_histogram(aes(x = Nv - vocab), fill = rgb(0.094,0.363,0.544, 0.5)) +
  ylab("") + xlab("") +
  ggtitle("Vocab. Diff.") +
  theme(axis.text.x = element_text(angle=45, hjust=1)) +
  theme(axis.text.y = element_blank())


plot(h1 + h2 + h3 + h4 + plot_layout(ncol = 4))

```


I used linear regression as a multiple correlation to examine which features of the LDA-DGP were associated with variability in the four parameters in Figure \@ref(fig:dtm-hist). This use of linear regression is not to model drivers of each parameter, rather to describe linear correlations holding other variables constant. For each of the three parameters of interest, I regressed them on the seed parameters of the LDA-DGP from Section 3.4.1.1 and their quadratics. The result, excluding regression constants, along with 95% confidence intervals are plotted in Figure \@ref(fig:zipf-regs). The top row includes both $\sum_{k=1}^K \alpha_k$ and $\left(\sum_{k=1}^K \alpha_k\right)^2$. The bottom row excludes both $\sum_{k=1}^K \alpha_k$ and $\left(\sum_{k=1}^K \alpha_k\right)^2$.

```{r zipf-regs, fig.cap = "Coefficients from regressions of four Zipf's law-related variables on parameters from the LDA-DGP and their quadratics, excluding intercept terms. The top row includes terms related to $\\boldsymbol\\alpha$ and the bottom row excludes them. Removing the $\\boldsymbol\\alpha$-related terms does not affect goodness of fit. The adjusted $R^2$ of the models in the top row are 0.733, 0.984, 0.866, and 0.920 respectively. The adjusted $R^2$ of the models in the bottom row are 0.736, 0.984, 0.866, and 0.920 respectively. The regressions for the Zipf constant and minimum word frequency are scaled by log 10.", fig.height = 5, fig.width = 7.5}

library(broom)


# format tables for plotting
zipf_reg <- list(
  zipf_coef = 
    lm(I(-1 * zipf2_coef) ~
         Nv +I(Nv^2) + 
         Nd + I(Nd^2) + 
         Nk + I(Nk^2) + 
         doc_length + I(doc_length^2) + 
         beta_sum + I(beta_sum^2) + 
         alpha_sum + I(alpha_sum^2), data = dtm_metrics) |>
    tidy() |>
    mutate(outcome = "gamma"),
  zipf_const = 
    lm(zipf2_const ~
         Nv +I(Nv^2) + 
         Nd + I(Nd^2) + 
         Nk + I(Nk^2) + 
         doc_length + I(doc_length^2) + 
         beta_sum + I(beta_sum^2) + 
         alpha_sum + I(alpha_sum^2), data = dtm_metrics) |>
    tidy() |>
    mutate(outcome = "log10(C)"),
  xmin = 
    lm(I(log10(zipf_xmin)) ~
         Nv +I(Nv^2) + 
         Nd + I(Nd^2) + 
         Nk + I(Nk^2) + 
         doc_length + I(doc_length^2) + 
         beta_sum + I(beta_sum^2) + 
         alpha_sum + I(alpha_sum^2), data = dtm_metrics) |>
    tidy() |>
    mutate(outcome = "xmin")
) |>
  bind_rows()

zipf_reg_no_alpha <- list(
  zipf_coef = 
    lm(I(-1 * zipf2_coef) ~
         Nv +I(Nv^2) + 
         Nd + I(Nd^2) + 
         Nk + I(Nk^2) + 
         doc_length + I(doc_length^2) + 
         beta_sum + I(beta_sum^2), data = dtm_metrics) |>
    tidy() |>
    mutate(outcome = "gamma"),
  zipf_const = 
    lm(zipf2_const ~
         Nv +I(Nv^2) + 
         Nd + I(Nd^2) + 
         Nk + I(Nk^2) + 
         doc_length + I(doc_length^2) + 
         beta_sum + I(beta_sum^2), data = dtm_metrics) |>
    tidy() |>
    mutate(outcome = "log10(C)"),
  xmin = 
    lm(I(log10(zipf_xmin)) ~
         Nv +I(Nv^2) + 
         Nd + I(Nd^2) + 
         Nk + I(Nk^2) + 
         doc_length + I(doc_length^2) + 
         beta_sum + I(beta_sum^2), data = dtm_metrics) |>
    tidy() |>
    mutate(outcome = "xmin")
) |>
  bind_rows()


# make some substitutions on term names for pretty plotting
zipf_reg <- 
  zipf_reg |>
  mutate(
    term = case_when(
      term == "Nv" ~ "V",
      term == "I(Nv^2)" ~ "V-sq.",
      term == "Nk" ~ "K",
      term == "I(Nk^2)" ~ "K-sq.",
      term == "Nd" ~ "D",
      term == "I(Nd^2)" ~ "D-sq.",
      term == "doc_length" ~ "Avg. Doc. Length",
      term == "I(doc_length^2)" ~ "Avg. Doc. Length-sq.",
      term == "beta_sum" ~ "Sum(eta)",
      term == "I(beta_sum^2)" ~ "Sum(eta)-sq.",
      term == "alpha_sum" ~ "Sum(alpha)",
      term == "I(alpha_sum^2)" ~ "Sum(alpha)-sq.",
      TRUE ~ term
    )
  )

zipf_reg_no_alpha <- 
  zipf_reg_no_alpha |>
  mutate(
    term = case_when(
      term == "Nv" ~ "V",
      term == "I(Nv^2)" ~ "V-sq.",
      term == "Nk" ~ "K",
      term == "I(Nk^2)" ~ "K-sq.",
      term == "Nd" ~ "D",
      term == "I(Nd^2)" ~ "D-sq.",
      term == "doc_length" ~ "Avg. Doc. Length",
      term == "I(doc_length^2)" ~ "Avg. Doc. Length-sq.",
      term == "beta_sum" ~ "Sum(eta)",
      term == "I(beta_sum^2)" ~ "Sum(eta)-sq.",
      term == "alpha_sum" ~ "Sum(alpha)",
      term == "I(alpha_sum^2)" ~ "Sum(alpha)-sq.",
      TRUE ~ term
    )
  )


p1 <- zipf_reg |>
  filter(outcome == "gamma" & term != "(Intercept)") |>
  mutate(
    ci_high = estimate + 1.96 * std.error,
    ci_low = estimate - 1.96 * std.error
  ) |>
  mutate(
    xlim_low = -1 * max(abs(c(ci_high, ci_low))),
    xlim_high = max(abs(c(ci_high, ci_low)))
  ) |>
  ggplot() +
  geom_bar(aes(x = estimate, y = term, fill = term), stat = "identity") +
  geom_errorbar(aes(
    y = term, 
    xmin = ci_low, 
    xmax = ci_high
  )) + 
  theme(legend.position = "none") +
  scale_y_discrete(limits = rev) +
  ylab("") + xlab("") +
  theme(axis.text.x = element_text(angle=45, hjust=1)) +
  ggtitle("Zipf Coefficient")

p2 <- zipf_reg |>
  filter(outcome == "log10(C)" & term != "(Intercept)") |>
  mutate(
    ci_high = estimate + 1.96 * std.error,
    ci_low = estimate - 1.96 * std.error
  ) |>
  mutate(
    xlim_low = -1 * max(abs(c(ci_high, ci_low))),
    xlim_high = max(abs(c(ci_high, ci_low)))
  ) |>
  ggplot() +
  geom_bar(aes(x = estimate, y = term, fill = term), stat = "identity") +
  geom_errorbar(aes(
    y = term, 
    xmin = ci_high, 
    xmax = ci_low
  )) + 
  theme(legend.position = "none") +
  scale_y_discrete(limits = rev) +
  ylab("") + xlab("") +
  theme(axis.text.y = element_blank()) +
  theme(axis.text.x = element_text(angle=45, hjust=1)) +
  ggtitle("Zipf Constant")

p3 <- zipf_reg |>
  filter(outcome == "xmin" & term != "(Intercept)") |>
  mutate(
    ci_high = estimate + 1.96 * std.error,
    ci_low = estimate - 1.96 * std.error
  ) |>
  mutate(
    xlim_low = -1 * max(abs(c(ci_high, ci_low))),
    xlim_high = max(abs(c(ci_high, ci_low)))
  ) |>
  ggplot() +
  geom_bar(aes(x = estimate, y = term, fill = term), stat = "identity") +
  geom_errorbar(aes(
    y = term, 
    xmin = ci_high, 
    xmax = ci_low
  )) + 
  theme(legend.position = "none") +
  scale_y_discrete(limits = rev) +
  ylab("") + xlab("") +
  theme(axis.text.y = element_blank()) +
  theme(axis.text.x = element_text(angle=45, hjust=1)) +
  ggtitle("Min. Word Freq.")


p4 <- zipf_reg_no_alpha |>
  filter(outcome == "gamma" & term != "(Intercept)") |>
  mutate(
    ci_high = estimate + 1.96 * std.error,
    ci_low = estimate - 1.96 * std.error
  ) |>
  mutate(
    xlim_low = -1 * max(abs(c(ci_high, ci_low))),
    xlim_high = max(abs(c(ci_high, ci_low)))
  ) |>
  ggplot() +
  geom_bar(aes(x = estimate, y = term, fill = term), stat = "identity") +
  geom_errorbar(aes(
    y = term, 
    xmin = ci_low, 
    xmax = ci_high
  )) + 
  theme(legend.position = "none") +
  scale_y_discrete(limits = rev) +
  ylab("") + xlab("") +
  theme(axis.text.x = element_text(angle=45, hjust=1)) +
  ggtitle("Zipf Coefficient")

p5 <- zipf_reg_no_alpha |>
  filter(outcome == "log10(C)" & term != "(Intercept)") |>
  mutate(
    ci_high = estimate + 1.96 * std.error,
    ci_low = estimate - 1.96 * std.error
  ) |>
  mutate(
    xlim_low = -1 * max(abs(c(ci_high, ci_low))),
    xlim_high = max(abs(c(ci_high, ci_low)))
  ) |>
  ggplot() +
  geom_bar(aes(x = estimate, y = term, fill = term), stat = "identity") +
  geom_errorbar(aes(
    y = term, 
    xmin = ci_high, 
    xmax = ci_low
  )) + 
  theme(legend.position = "none") +
  scale_y_discrete(limits = rev) +
  ylab("") + xlab("") +
  theme(axis.text.y = element_blank()) +
  theme(axis.text.x = element_text(angle=45, hjust=1)) +
  ggtitle("Zipf Constant")

p6 <- zipf_reg_no_alpha |>
  filter(outcome == "xmin" & term != "(Intercept)") |>
  mutate(
    ci_high = estimate + 1.96 * std.error,
    ci_low = estimate - 1.96 * std.error
  ) |>
  mutate(
    xlim_low = -1 * max(abs(c(ci_high, ci_low))),
    xlim_high = max(abs(c(ci_high, ci_low)))
  ) |>
  ggplot() +
  geom_bar(aes(x = estimate, y = term, fill = term), stat = "identity") +
  geom_errorbar(aes(
    y = term, 
    xmin = ci_high, 
    xmax = ci_low
  )) + 
  theme(legend.position = "none") +
  scale_y_discrete(limits = rev) +
  ylab("") + xlab("") +
  theme(axis.text.y = element_blank()) +
  theme(axis.text.x = element_text(angle=45, hjust=1)) +
  ggtitle("Min. Word Freq.")

# add some heaps stuff here
heaps_reg <- 
  list(
    with_alpha =  
      lm(I(Nv - vocab) ~
           Nv +I(Nv^2) + 
           Nd + I(Nd^2) + 
           Nk + I(Nk^2) + 
           doc_length + I(doc_length^2) + 
           beta_sum + I(beta_sum^2) + 
           alpha_sum + I(alpha_sum^2), data = dtm_metrics) |>
      tidy() |>
      mutate(alpha = "y"),
    no_alpha = 
      lm(I(Nv - vocab) ~
           Nv +I(Nv^2) + 
           Nd + I(Nd^2) + 
           Nk + I(Nk^2) + 
           doc_length + I(doc_length^2) + 
           beta_sum + I(beta_sum^2), data = dtm_metrics) |>
      tidy() |>
      mutate(alpha = "n")
  ) |>
  bind_rows() |>
  mutate(
    term = case_when(
      term == "Nv" ~ "V",
      term == "I(Nv^2)" ~ "V-sq.",
      term == "Nk" ~ "K",
      term == "I(Nk^2)" ~ "K-sq.",
      term == "Nd" ~ "D",
      term == "I(Nd^2)" ~ "D-sq.",
      term == "doc_length" ~ "Avg. Doc. Length",
      term == "I(doc_length^2)" ~ "Avg. Doc. Length-sq.",
      term == "beta_sum" ~ "Sum(eta)",
      term == "I(beta_sum^2)" ~ "Sum(eta)-sq.",
      term == "alpha_sum" ~ "Sum(alpha)",
      term == "I(alpha_sum^2)" ~ "Sum(alpha)-sq.",
      TRUE ~ term
    )
  )


p7 <- heaps_reg |>
  filter(alpha == "y" & term != "(Intercept)") |>
  mutate(
    ci_high = estimate + 1.96 * std.error,
    ci_low = estimate - 1.96 * std.error
  ) |>
  mutate(
    xlim_low = -1 * max(abs(c(ci_high, ci_low))),
    xlim_high = max(abs(c(ci_high, ci_low)))
  ) |>
  ggplot() +
  geom_bar(aes(x = estimate, y = term, fill = term), stat = "identity") +
  geom_errorbar(aes(
    y = term, 
    xmin = ci_low, 
    xmax = ci_high
  )) + 
  theme(legend.position = "none") +
  scale_y_discrete(limits = rev) +
  ylab("") + xlab("") +
  theme(axis.text.x = element_text(angle=45, hjust=1)) +
  theme(axis.text.y = element_blank()) +
  ggtitle("Vocab. Diff.")

p8 <- heaps_reg |>
  filter(alpha == "n" & term != "(Intercept)") |>
  mutate(
    ci_high = estimate + 1.96 * std.error,
    ci_low = estimate - 1.96 * std.error
  ) |>
  mutate(
    xlim_low = -1 * max(abs(c(ci_high, ci_low))),
    xlim_high = max(abs(c(ci_high, ci_low)))
  ) |>
  ggplot() +
  geom_bar(aes(x = estimate, y = term, fill = term), stat = "identity") +
  geom_errorbar(aes(
    y = term, 
    xmin = ci_low, 
    xmax = ci_high
  )) + 
  theme(legend.position = "none") +
  scale_y_discrete(limits = rev) +
  ylab("") + xlab("") +
  theme(axis.text.x = element_text(angle=45, hjust=1)) +
  theme(axis.text.y = element_blank()) +
  ggtitle("Vocab. Diff.")

plot(
  (p1 + p2 + p3 + p7 + p4 + p5 + p6 + p8) + 
    plot_layout(ncol = 4) + 
    plot_annotation(title = "Regression Coefficients With (top) and Without (bottom) Sum(alpha)")
)

```

The magnitude of $\boldsymbol\alpha$ has on the four Zipf variables appears to be negligible. In each case, the coefficient on $\sum_{k = 1} ^ K \alpha_k$ show a moderately-sized mean response on the outcome. However, in all three cases, the 95% confidence interval is extremely large. Removing both $\sum_{k=1}^K \alpha_k$ and $\left(\sum_{k=1}^K \alpha_k\right)^2$ does not lower the $R^2$ of any of the regressions, nor appreciably change the coefficients associated with other variables.

The number of topics, $K$, is positively associated with the Zipf coefficient, $\gamma$, and negatively associated with the minimum frequency of words used to estimate the Zipf parameters. In other words, more topics is associated with a steeper slope of the Zipf curve in log-log space. And more topics is also associated with Zipf's law "falling" off for low frequency words deeper into the tail. In plain speech: a corpus with more topics seems to express Zipf's law more strongly.

The magnitude of $\boldsymbol\eta$ has a similar relationship as $K$ to $\gamma$ and the minimum frequency of words used to estimate the Zipf parameters. In other words, Having topics that are more highly correlated with each other (tuned by the magnitude of $\boldsymbol\eta$) is associated with a corpus expressing Zipf's law more strongly.

The Zipf constant, $C$, is most strongly associated by the total volume of words in a corpus. The total volum of words in a corpus is the average context length multipled by the total number of contexts (i.e., Avg. Doc Length times $D$). In a separate regression not reported here, the average context length and number of contexts alone account for 95% of the variance in $\log_{10} C$. This makes intuitive sense as $C$ acts only to scale Zipf's law to the corpus on which it is applied.

Interestingly, the number of unique words in a corpus, $V$, does not appear to have a strong correlation to Zipf's law.

Finally, having more topics, and to a lesser degree a larger magnitude $\boldsymbol\eta$ and longer contexts, are associated with observing more vocabulary words. It makes intuitive sense that longer contexts would be associated with observing more vocabulary words; it's simply a larger sample. A larger magnitude $\boldsymbol\eta$ means that there is less variability in vocabulary between topics, which would make the tail end of the Zipf distribution less sparse. This, again, would make it more likely that one would observe a larger vocabulary. 


### Describing LDA Model Misspecification
Since we know the process that created a synthetic data set, we can measure the difference between a perfectly-specified model and misspecified models. 

#### Modeling Choices and Evaluation Methods

I took a random sample of 1,024 of the simulated data sets[^tookasample] and on each fit five LDA models, one perfectly specified according to the parameters that generated the data and four misspecified models. The misspecified models misspecify $K$, $\sum_{k = 1}^K \alpha_k$, $\sum_{v = 1}^V \eta_v$, and the shape of $\boldsymbol\eta$, making it flat. For each model, the Gibbs sampler was run for 200 iterations, 150 of which were burn in iterations. The final posterior is the average over the last 50 iterations. For the hyper parameters $K$, $\sum_{k=1}^K \alpha_k$, and $\sum_{v=1}^V \eta_v$, misspecifications are chosen by random sample of the available options in Section 3.4.1, excluding the correct value. For example, to misspecify the number of topics in a model where the true value is $K=100$, the estimated $K$ is selected at random from $\{25, 50, 200\}$. As a result, approximatly half of the misspecified models will have the key hyper parameter too small and half will be too big.

[^tookasample]: These results are preliminary. I will perform the analysis on the full set of 4,096 data sets in the final version of this dissertation.

<!---
When fitting LDA models using Gibbs sampling, it is useful to see if the chain has converged. To do this, I use Geweke's convergence statistic [@geweke1991evaluating] on the log likelihood of the estimated model over the last 50 iterations of the chain (the area over which I average the posteriors). The test statistic is a standard Z-score on the difference between two sample means of two partitions of the chain, generally the first 10 percent and last 50 percent. If the two partitions are not significantly different, one can assume that the chain has converged. I compare the absolute values of the Geweke test statistic between the perfectly specified and misspecified models to estimate the degree to which model misspecification affects convergence. The absolute value is used because the Geweke test statistic is drawn from a symmetric two-tailed distribution where extremes have both positive and negative values. 

In addition to the Geweke statistic, I also compare the following between correctly-specified and misspecified models:
--->

I compare the following between correctly-specified and misspecified models:

* Log Likelihood, averaged over the last 50 Gibbs iterations, of perfectly specified to misspecified models. The log likelihood calculation follows that used in [@chen2015warplda].
* Coefficient of determination, or $R^2$. This statistic calculates the proportion of variability in the data that has been accounted for in the fit model. This statistic and a study of its properties is the next chapter of this dissertation.
* Probabilistic coherence (see Appendix 5)---or just "coherence"---averaged over all topics in a model. 
* Prevalence of each topic, averaged across all topics in the model. The prevalence of topic $k$ is calculated with $\sum_{d = 1} ^ D N_d \cdot \theta_{d,k} \cdot100$.

In the comparing correctly specified and misspecified models, I use a paired t-statistic to calculate 95% confidence intervals in the difference between the correct (i.e., "untreated") and misspecified (i.e., "treated") groups. This paired t-statistic confidence interval is applied to the Geweke statistics, log likelihood, $R^2$, probabilistic coherence, and prevalence. 

#### Results
Figure \@ref(fig:model-plots) plots the mean differences and 95% confidence intervals comparing the perfectly-specified model and each of the models where a single hyper parameter was misspecified. Note that because only one hyper parameter was misspecified at a time, that these mean differences represent a causal effect of single hyper parameter misspecification. This analysis does not evaluate the effects of multiple misspecifications at once.

Misspecifying the number of topics has the largest effects on model goodness of fit, whether measured with log likelihood or $R^2$. This may reflect the nature of the goodness of fit metrics. All else constant, a model with more topics will have a lower likelihood, reflecting a larger sample space. And, as demonstrated in Chapter 4, $R^2$ increases with the number of estimated topics, seemingly independently of the true number of topics. However, it is worth noting that having too many topics does not increase $R^2$ nearly as much as having too few topics decreases it. 

Misspecifying the number of topics is seemingly the only misspecification that affects the average prevalence of topics. This result is intuitive and similarly a reflection of the properties of prevalence. Having more estimated topics will spread prevalence over more topics, decreasing the average.

Misspecifying $\boldsymbol\eta$ has mixed effects. A flat $\boldsymbol\eta$ seems to improve the log likelihood of the model and mean topic coherence, yet it decreases $R^2$. Specifying the magnitude of $\boldsymbol\eta$ to be too large has a negative effect on log likelihood, $R^2$, and coherence. Yet specifying the magnitude of $\boldsymbol\eta$ to be too small seems to have little effect save a moderate boost in $R^2$.

Misspecifying $\boldsymbol\alpha$ does not seem to have much of an effect at all. 

<!---
Interpreting differences in the absolute value of the Geweke statistic is complex and may not be valid in this context. When a chain has converged, the Geweke statistic should be small in absolute value. So, decreases in the absolute value of the Geweke statistic show models closer to convergence. However, only about 25% of the models showed signs of having converged according to the Geweke diagnostic. With those caveats, 

In theory, a smal

1. misspecifying the number of topics has the greatest impact on goodness of fit
2. specification of eta has an impact as well, but how is mixed (flat vs pl)
3. misspecification of alpha does not seem to have much of an impact, counter result to other studies -> needs much more rigorous examination than I do here
4. Flat eta seems to really improve coherence -> overfit on data?
5. K is only thing that affects average prevalence in ways that are very intuitive
6. Looks like misspecification leads to more robust convergence? What?
--->


```{r model-plots, fig.cap = "Mean differences (with 95% confidence intervals) in the evaluation metrics between perfectly-specified models and misspecified models. The misspecified hyper parameter is on the x-axis.  ", fig.width = 7, fig.height = 5}

# The Geweke statistic is measured in absolute value since because it is a statistic with a two-tailed test distribution. Histograms of the Geweke Statistic are plotted on the top right. The vertical lines ar at +/- 1.96, between which a model might be considered to have converged at the 0.05 significance level.

t_tests <- read_rds("data-derived/zipf-analysis/model-t-tests.rds")

model_metrics <- read_rds("data-derived/zipf-analysis/model-metrics.rds")

times_help <- function(x, y) x * y

ll_plot <- 
  t_tests |> 
  filter(outcome == "ll") |> 
  ggplot() + 
  geom_bar(aes(x = parameter, y = mean_diff, fill = factor(parameter)), stat = "identity") + 
  geom_errorbar(aes(x = parameter, ymin = ci_95_low, ymax = ci_95_high)) + 
  geom_hline(yintercept = 0, lty = 2) +
  xlab("") + ylab("") +
  ggtitle("Log Likelihood") +
  theme(legend.position = "none") +
  theme(axis.text.x = element_blank()) +
  ylim(
    c(
      t_tests$ci_95_low[t_tests$outcome == "ll"], 
      t_tests$ci_95_high[t_tests$outcome == "ll"]
    ) |> 
      abs() |> max() %>% times_help(-1),
    c(
      t_tests$ci_95_low[t_tests$outcome == "ll"], 
      t_tests$ci_95_high[t_tests$outcome == "ll"]
    ) |> 
      abs() |> max()
  )

r2_plot <- 
  t_tests |> 
  filter(outcome == "r2") |> 
  ggplot() + 
  geom_bar(aes(x = parameter, y = mean_diff, fill = factor(parameter)), stat = "identity") + 
  geom_errorbar(aes(x = parameter, ymin = ci_95_low, ymax = ci_95_high)) + 
  geom_hline(yintercept = 0, lty = 2) +
  xlab("") + ylab("") +
  ggtitle("R-Squared") +
  theme(legend.position = "none") +
  theme(axis.text.x = element_blank()) +
  ylim(
    c(
      t_tests$ci_95_low[t_tests$outcome == "r2"], 
      t_tests$ci_95_high[t_tests$outcome == "r2"]
    ) |> 
      abs() |> max() %>% times_help(-1),
    c(
      t_tests$ci_95_low[t_tests$outcome == "r2"], 
      t_tests$ci_95_high[t_tests$outcome == "r2"]
    ) |> 
      abs() |> max()
  )

mean_coherence_plot <- 
  t_tests |> 
  filter(outcome == "mean_coherence") |> 
  ggplot() + 
  geom_bar(aes(x = parameter, y = mean_diff, fill = factor(parameter)), stat = "identity") + 
  geom_errorbar(aes(x = parameter, ymin = ci_95_low, ymax = ci_95_high)) + 
  geom_hline(yintercept = 0, lty = 2) +
  xlab("") + ylab("") +
  ggtitle("Mean Coherence") +
  theme(legend.position = "none") +
  theme(axis.text.x = element_text(angle=45, hjust=1)) +
  ylim(
    c(
      t_tests$ci_95_low[t_tests$outcome == "mean_coherence"], 
      t_tests$ci_95_high[t_tests$outcome == "mean_coherence"]
    ) |> 
      abs() |> max() %>% times_help(-1),
    c(
      t_tests$ci_95_low[t_tests$outcome == "mean_coherence"], 
      t_tests$ci_95_high[t_tests$outcome == "mean_coherence"]
    ) |> 
      abs() |> max()
  )

mean_prevalence_plot <- 
  t_tests |> 
  filter(outcome == "mean_prevalence") |> 
  ggplot() + 
  geom_bar(aes(x = parameter, y = mean_diff, fill = factor(parameter)), stat = "identity") + 
  geom_errorbar(aes(x = parameter, ymin = ci_95_low, ymax = ci_95_high)) + 
  geom_hline(yintercept = 0, lty = 2) +
  xlab("") + ylab("") +
  ggtitle("Mean Prevalence") +
  theme(legend.position = "none") +
  theme(axis.text.x = element_text(angle=45, hjust=1)) +
  ylim(
    c(
      t_tests$ci_95_low[t_tests$outcome == "mean_prevalence"], 
      t_tests$ci_95_high[t_tests$outcome == "mean_prevalence"]
    ) |> 
      abs() |> max() %>% times_help(-1),
    c(
      t_tests$ci_95_low[t_tests$outcome == "mean_prevalence"], 
      t_tests$ci_95_high[t_tests$outcome == "mean_prevalence"]
    ) |> 
      abs() |> max()
  )

geweke_plot <- 
  t_tests |> 
  filter(outcome == "geweke") |> 
  ggplot() + 
  geom_bar(aes(x = parameter, y = mean_diff, fill = factor(parameter)), stat = "identity") + 
  geom_errorbar(aes(x = parameter, ymin = ci_95_low, ymax = ci_95_high)) + 
  geom_hline(yintercept = 0, lty = 2) +
  xlab("") + ylab("") +
  ggtitle("Geweke Statistic") +
  theme(legend.position = "none") +
  theme(axis.text.x = element_text(angle=45, hjust=1)) +
  ylim(
    c(
      t_tests$ci_95_low[t_tests$outcome == "geweke"], 
      t_tests$ci_95_high[t_tests$outcome == "geweke"]
    ) |> 
      abs() |> max() %>% times_help(-1),
    c(
      t_tests$ci_95_low[t_tests$outcome == "geweke"], 
      t_tests$ci_95_high[t_tests$outcome == "geweke"]
    ) |> 
      abs() |> max()
  )

geweke_density <- model_metrics |>
  mutate(
    model_type = case_when(
      Nk == est_k & beta_sum == est_eta_sum & alpha_sum == est_alpha_sum &! est_flat_eta ~ "correct",
      Nk < est_k ~ "K big",
      Nk > est_k ~ "K small",
      beta_sum < est_eta_sum & ! est_flat_eta ~ "eta sum big",
      beta_sum > est_eta_sum & ! est_flat_eta ~ "eta sum small",
      est_flat_eta ~ "eta flat",
      alpha_sum < est_alpha_sum ~ "alpha sum big",
      alpha_sum > est_alpha_sum ~ "alpha sum small",
      TRUE ~ "what happened?"
    ) 
  ) |>
  ggplot() +
  geom_density(aes(x = geweke_stat, fill = model_type, color = model_type), alpha = 0.3) +
  geom_vline(xintercept = c(-1, 1) * 1.96) + 
  ggtitle("Geweke Statistic") + 
  ylab("") + xlab("") +
  theme(axis.text.y = element_blank()) +
  theme(axis.text.x = element_blank()) +
  theme(legend.position = "none")


part_r2_plot <- 
  t_tests |> 
  filter(outcome == "partial_r2") |> 
  ggplot() + 
  geom_bar(aes(x = parameter, y = mean_diff, fill = factor(parameter)), stat = "identity") + 
  geom_errorbar(aes(x = parameter, ymin = ci_95_low, ymax = ci_95_high)) + 
  geom_hline(yintercept = 0, lty = 2) +
  xlab("") + ylab("") +
  ggtitle("Mean Partial R-sq.") +
  theme(legend.position = "none") +
  theme(axis.text.x = element_text(angle=45, hjust=1)) +
  ylim(
    c(
      t_tests$ci_95_low[t_tests$outcome == "partial_r2"], 
      t_tests$ci_95_high[t_tests$outcome == "partial_r2"]
    ) |> 
      abs() |> max() %>% times_help(-1),
    c(
      t_tests$ci_95_low[t_tests$outcome == "partial_r2"], 
      t_tests$ci_95_high[t_tests$outcome == "partial_r2"]
    ) |> 
      abs() |> max()
  )

lambda_coh_plot <- 
  t_tests |> 
  filter(outcome == "mean_lambda_coherence") |> 
  ggplot() + 
  geom_bar(aes(x = parameter, y = mean_diff, fill = factor(parameter)), stat = "identity") + 
  geom_errorbar(aes(x = parameter, ymin = ci_95_low, ymax = ci_95_high)) + 
  geom_hline(yintercept = 0, lty = 2) +
  xlab("") + ylab("") +
  ggtitle("Mean Lambda Coherence") +
  theme(legend.position = "none") +
  theme(axis.text.x = element_text(angle=45, hjust=1)) +
  ylim(
    c(
      t_tests$ci_95_low[t_tests$outcome == "mean_lambda_coherence"], 
      t_tests$ci_95_high[t_tests$outcome == "mean_lambda_coherence"]
    ) |> 
      abs() |> max() %>% times_help(-1),
    c(
      t_tests$ci_95_low[t_tests$outcome == "mean_lambda_coherence"], 
      t_tests$ci_95_high[t_tests$outcome == "mean_lambda_coherence"]
    ) |> 
      abs() |> max()
  )




# pw <- 
#   (ll_plot + r2_plot + geweke_density + 
#      mean_coherence_plot + mean_prevalence_plot + geweke_plot) +
#   plot_annotation(title = "Mean Differences") 

pw <- 
  (ll_plot + 
     r2_plot + 
     mean_coherence_plot + 
     mean_prevalence_plot +
     part_r2_plot +
     lambda_coh_plot) +
  plot_annotation(title = "Mean Differences") 

plot(pw)

```


## Discussion
This chapter demonstrates how to generate synthetic corpora using the LDA-DGP that are consistent with the key empirical statistical laws of language. This enables researchers to engage in more principled simulation studies for LDA as a model for human language. Such simulations are used throughout the remainder of this dissertation.

Two analyses are performed here. The first is describing which aspects of the LDA-DGP are associated with parameter values for Zipf's law and (by proxy) Heaps's law. The second systematically misspecifies LDA models and measures the causal effects of hyper parameter misspecification for the generated data sets.

To align the synthetic corpus's word frequencies to the real corpus's word frequencies in Figure \@ref(fig:compare-hypers), I did not remove stop words from the NIH corpus. All synthetic corpora in this chapter (and dissertation) produced with the LDA-DGP include stop words. However in practice, researchers tend to remove stop words prior to fitting an LDA model. This presents several options for future consideration. If we use synthetic data studies to examine the properties of topic models leaving stop words in (as is done here), researchers would need to include stop words in their models to be consistent. This is in line with [@schofield2017pulling], which shows that stop words may be removed after modeling to aid human interpretation while modeling the _whole_ corpus. An alternative of the method I use here, using the more general Zipf-Mandlebrodt law may allow the production of synthetic corpora where stop words have been removed [@mandelbrot1965information].

The studies here demonstrate that $\boldsymbol\eta$ is more important than previously appreciated. This chapter also brings some evidence that $\boldsymbol\alpha$ is not as important as is conventionally understood. Specifically, the results about $\boldsymbol\alpha$ and $\boldsymbol\eta$ presented here are counter the results of [@wallach2009rethinking]. I view this as evidence that more research is needed, rather than evidence overturning the view on LDA's priors. The studies in this chapter are preliminary. I did not generate synthetic corpora using asymmetric specifications of $\boldsymbol\alpha$, nor did I seek to validate patterns of misspecification on LDA models fit on real data. I leave such efforts for future work and only note that there is much more to be understood about prior specification for LDA models.

Misspecifying K has the most consistent overall impact on posterior statistics. The instinct of many to worry about it is right. We still need future research to find principled ways of setting the number of topics. However by linking Zipf's (and thus Heaps's) law to the LDA-DGP, such studies are now significantly closer to being available. We can now create synthetic data where the true data generation process is known for LDA, enabling a more rigorous examination of modeling effects and best practices. 

```{r clear-dgp-data}
rm(list = ls())
```

