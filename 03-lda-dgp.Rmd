# Linking Zipf's law to the LDA-DGP {#ldadgp}

LDA is a latent variable model and, as a result, it can be challenging to study. No ground truth exists against which to compare models and methods. Selecting hyperparameters when building models is notoriously difficult as little formal guidance exists to guide a researcher's choices. Lack of ground truth complicates development of new metrics and methods. How is one to know if a new method is an improvement or a new metric insightful?

Fortuately, LDA models a generative process.This allows researchers to generate synthetic data sets by sampling from the LDA data generating process (LDA-DGP) where they have chosen $K$, $\boldsymbol\alpha$, and $\boldsymbol\eta$ themselves. They then may use these simulated data sets to compare a model against a synthetic ground truth. Such "simulation studies" have a lengthy history in the statistical literature, going back to 1975 or further [@Hoaglin1975]. Yet to be valid, synthetic data from the LDA-DGP must conform to empirical statistical laws of language such as Zipf's and Heaps's laws. 

In this chapter I examine the LDA-DGP analytically and empirically to do the following:

1. Link Zipf's law to the LDA-DGP,
2. Describe the roll of the magnitudes of $\boldsymbol\alpha$ and $\boldsymbol\eta$ in tuning the correlations between documents and topics, and
3. Describe the effects of misspecification of hyperparameters on the posterior of fit LDA models.

I find that [describe findings in brief here].

## Related Work

Work related to studying the LDA-DGP pulls from two seemingly disparate fields: topic modeling and complex systems theory. Complex systems theory deals in part with the emergence of power law distributions---as exemplified by some empirical laws of language---from complex systems [@cioffi2008power]. 

### Simulation Studies for LDA

Synthetic corpora appear commonly in topic modeling research. Shi et al. [@shi2019eval] list 14 works using synthetic corpora to study topic models. (See Table S3 in [@shi2019eval].) Most use some flavor of the LDA-DGP and focus on only one or two aspects of comparison between model and ground truth in the synthetic data set. Boyd-Graber, Hu, and Mimno suggest the use of semi-synthetic data---i.e., drawing simulations from the posterior---to capture properties of natural language [@boydgraber2017applications]. I am unaware of any work linking empirical laws of language to the LDA-DGP. Shi et al. link their simulation method to Zipf's law but it does not use the LDA-DGP to produce its simulations [@shi2019eval; @shi2019thesis]. 

Shi et al. go further than others and argue that use of synthetic corpora is "a principled approach" to evaluating topic models [@shi2019eval; @shi2019thesis]. Their approach does reproduce Zipf's law of language. Yet it does not use the LDA-DGP. It may represent a principled approach to studying probabilistic topic models with simulated data, but it is a parallel path to the one in this chapter. Using the LDA-DGP specifically allows for stronger statements related to pathological misspecification of LDA models. e.g., "Under the true model, then I would expect to see outcome A. Instead I see outcome B. Therefore, my model must be misspecified."[^linearanalogue] 

[^linearanalogue]: An analogue from linear regression might be, "under the true model, residuals are independent and identically distributed (_i.i.d._) Gaussian with mean zero. The residuals of my model are not _i.i.d._ Gaussian with mean zero. Therefore, my model must be misspecified."

### Empirical Language Laws

Altmann and Gerlach describe 9 universal laws purported to describe statistical regularities in human language [@altmann2015laws]. These laws are Zipf's [@zipf1949], Heaps's [@egghe2007untangling], Taylor's [@gerlach2014], Menzerath-Altmann [@altmann1980prolegomena], Recurrence [@zipf1949], Long-range correlation [@damerau1973tests], Entropy scaling [@altmann2015laws], Information content [@zipf1949], and various network topology laws [@altmann2015laws]. 

Of these laws, Zipf's and Heaps's laws are the most well known and relevant to LDA. The laws of Menzerath-Altmann, Recurrence, Long-range correlation, and Information content refer to properties of sub-words (e.g. length to information content), order of words, or proximity between words. LDA-DGP does not purport to model any of these. The law of Entropy scaling links entropy---in the information theoretic sense [@shannon2001mathematical]---to the number of words in a block of text. The LDA-DGP may or may not be able to reproduce this law. Similarly, some network views of a corpus may or may not apply to the LDA-DGP. Both may warrant future exploration but are less directly macroscopic properties of a corpus. 

Taylor's law may be relevant to the LDA-DGP but I leave its examination to future work as Zipf's and Heaps's laws are most commonly known as linguistic laws. Taylor's law was originally posed in the context of ecology [@taylor1961aggregation]. Its linguistic interpretation is that the standard deviation of the total number of words is proportional to the power of the mean of the total number of words in a corpus. Gerlach and Altmann explore the relationships between Zipf's, Heaps's, and Taylor's laws in the context of topic modeling [@gerlach2014]. Instead of using the LDA-DGP directly, they examine its asymptotic form, which is a Poisson process. 

#### Zipf's law
Zipf's law states that the frequency of a word is inversely proportional to the power of its frequency-rank. Zipf's law is not unique to any language as it appears to apply to all of them [@cancho2003]. Zipf's law has also been applied to the sizes of cities [@arshad2018zipf], casualties in armed conflict [@gillespie2015], and more. Zipf's law is a statement of a word's frequency and its rank. Yet if word frequencies in a corpus are plotted as a histogram, the power law relationship holds [@sole2008].

Empirical distributions of Zipf's law for large corpora demonstrate a relationship somewhat inconsistent with that predicted by Zipf's law. Some have proposed that the frequency-to-rank relationship is actually a set of broken power laws with one parametarization for the head of the distribution, another for the body, and a third parametarization for the tail. Yet, Ha et al. find that this is a trick of tokenization. "Language is not made of individual words but also consists of phrases of 2, 3 and more words, usually called n-grams for n=2, 3, etc." When including n-grams in both English and Chinese corpora, Ha et al. find that Zipf's law holds through the tail [@ha2002zipf]. Mandelbrot developed a generalization of Zipf's law that accounts for behavior at the head of the distribution [@mandelbrot1965information].

Formally, Zipf's law is

\begin{align}
  F(r) \propto r^{-\gamma} \text{ for } \gamma \geq 1, r > 1
\end{align}

where $r$ is a word's rank, $F(r)$ is the frequency of a word's rank, and $\gamma$ is a parameter to be estimated. For human language $\gamma \approx 1$ [@cancho2003] and can be found through maximum likelihood estimation [@gillespie2015]. Altmann and Gerlach find $1.03 \leq \gamma \leq 1.58$ depending on the corpus and estimation method [@altmann2015laws]

Goldwater et al. explore the relationship between Zipf's law and then standard statistical models of language [@goldwater2011zipf]. They develop a framework for producing power law word frequencies in two stages. Critically, they link this framework to several models closely related to LDA but do not extend it to the LDA-DGP itself.

#### Heaps's law
Heaps's law states that the number of unique words in a corpus, $V$, scales sub-linearly with the total number of words in a corpus, $N$ [@heaps1978information]. Under mild assumptions, Heaps's law is asymptotically equivalent to Zipf's law [@kornai1999zipf]. Unlike Zipf's law, it is an increasing power law.

\begin{align}
  V \propto N^\delta \text{ for } N > 1, 0 < \delta < 1
\end{align}

There are several ways to compute Heaps's law for a single corpus. Altmann and Gerlach use two methods: the first computes $V$ and $N$ for each document in the corpus and the second progresses over each word in a corpus calculating new values of $V$ and $N$ as each word is added. The latter approach works for single documents of sufficient length as well, such as a book [@altmann2015laws]. 

## The LDA Data Generating Process (LDA-DGP)

Assuming there are $D$ contexts, $K$ topics, $V$ unique words, $N$ total words and $N_d$ words in the $d$-th context, the LDA-DGP is as follows. For each word, $n$, in context $d$: 

1. Generate $\boldsymbol{B}$ by sampling $K$ topics $\boldsymbol\beta_k \sim \text{Dirichlet}(\boldsymbol\eta), \forall k \in \{1,2,...,K\}$
2. Generate $\boldsymbol\Theta$ by sampling $D$ documents $\boldsymbol\theta_d \sim \text{Dirichlet}(\boldsymbol\alpha), \forall d \in \{1,2, ..., D\}$
3. Then for each context, $d$
    1. Draw topic $z_{d,n}$ from $\text{Multinomial}(\boldsymbol\theta_d)$
    2. Draw word $w_{d,n}$ from $\text{Multinomial}(\boldsymbol\beta_{z_{d,n}})$
    3. Repeat 1. and 2. $N_d$ times.

Note that context $d$ has $N_d$ words $\forall d \in \{1, 2, ..., D\}$ and that the total number of words in the corpus is $N = \sum_{d=1}^D N_d$.

## Linking the LDA-DGP to Zipf's law
Zipf's law describes the relative frequencies between words in a corpus. Therefore, the expected term frequency distribution of a corpus drawn from the LDA-DGP should describe the relationship between Zipf's law and the LDA-DGP. This relationship is derived in Appendix 3 and gives the following relationship:

\begin{align}
    \mathbb{E}(w_v) &=
        N \frac{\eta_v}{\sum_{v = 1}^V \eta_v} \label{eq:1}
\end{align}

where $w_v$ is the total number of times word $v$ was sampled across all $D$ contexts.

(\ref{eq:1}) implies that the relative frequencies of words $i$ and $j$ are given by their ranks in a power law relationship. In other words, the shape of $\boldsymbol\eta$ is a power law, given by Zipf's law. This makes intuitive sense. Given Zipf's law, a single topic should be power law-shaped as an author could conceivably write only about a single topic.

No elements of $\boldsymbol\alpha$ appear in (\ref{eq:1}). As shown in Appendix 3, the $\alpha_i$'s sum and factor out of the derivation. That the average distribution of topics across documents is not tied to Zipf's law also makes intuitive sense. Word frequencies would not give us insight into the distribution of topics in a corpus

the expected term frequency of a corpus drawn from the LDA-DGP depends only on and is proportional to $\boldsymbol\eta$. The Dirichlet parameter $\boldsymbol\eta$ shapes the distribution of words over topics. Then, to generate corpora that follow Zipf’s law, the entries of $\boldsymbol\eta$ must themselves follow a power law distribution. The distribution of topics within documents does not matter as the $\alpha_i$'s factor out of the derivation. This makes some ntuitive sense. An author can theoretically write an entire corpus of documents about only one topic or an author may write a corpus of documents with an even distribution of topics. In either case the aggregate word frequencies must follow Zipf's law.

However, other documents could follow other topic distributions. Yet the language

This result is consistent with
the framework laid out by Goldwater et al. (2011). The Dirichlet multinomial
model - of which LDA is an application - is a special case of their framework.
In this framework, the Dirichlet prior has a parameter following a power law.


They did not put this in the specific form of LDA, however, as is done in this
paper.
Using this specification of the Dirichlet multinomial it is now possible to
generate corpora with the statistical properties of natural language within an

LDA framework. This facilitates exploring the properties of LDA under dif-
ferent conditions and the development of metrics aiding in model specification

and post-estimation diagnostics. However, further questions remain. How het-
erogeneous or homogenous are topics in real data? For example, is it sufficient

to draw topics from a single asymptotic Dirichlet distribution? Or, is there a
limit to the topical heterogeneity present in a real corpus if sensible estimates
are to be obtained by an LDA model?
The prior for the distribution of topics within documents does not need to
follow a Dirichlet distribution at all. The proof in the Appendix shows that
the elements of phi sum to 1 and factor out in expectation. A non-Dirichlet

distribution would only scale the expected term frequency distribution. How-
ever, since power laws are scale free there would be no substantive effect on

the term frequency.

```{r compare-hypers, fig.cap = "this is a caption", fig.width = 7, fig.height = 2.5}

library(tidyverse)
library(patchwork)

nih_plotmat <- read_rds("data-derived/zipf-analysis/nih-plotmat.rds")

p1 <- nih_plotmat |> ggplot() + 
  geom_line(aes(x = rank, y = pl_pl), color = NA, lty = 4, lwd = 1.25) + 
  geom_line(aes(x = rank, y = npl_npl), color = "#b2df8a", lty = 2, lwd = 1.25) +
  geom_line(aes(x = rank, y = pl_npl), color = "#a6cee3", lty = 4, lwd = 1.25) + 
  ylim(1, 10000) +
  scale_x_continuous(trans='log10') +
  scale_y_continuous(trans='log10') +
  ylab("Frequency") +
  xlab("Rank") +
  ggtitle("Symmetric Eta")

p2 <- nih_plotmat |> ggplot() + 
  geom_line(aes(x = rank, y = pl_pl), color = "#1f78b4", lty = 4, lwd = 1.25) + 
  geom_line(aes(x = rank, y = npl_pl), color = "#33a02c", lty = 2, lwd = 1.25) + 
  ylim(1, 10000) + 
  scale_x_continuous(trans='log10') +
  scale_y_continuous(trans='log10') +
  ylab("") +
  xlab("Rank") + 
  ggtitle("Power Law Eta")

p3 <- nih_plotmat |> ggplot() + 
  geom_line(aes(x = rank, y = nih), color = "#d95f02", lwd = 1.25) + 
  geom_line(aes(x = rank, y = pl_pl), color = "#1b9e77", lty = 4, lwd = 1.25) + 
  ylim(1, 10000) +
  scale_x_continuous(trans='log10') +
  scale_y_continuous(trans='log10') +
  ylab("") +
  xlab("Rank") + 
  ggtitle("NIH Corpus")


plot(p1 + p2 + p3)

```

Figure \@ref(fig:compare-hypers) plots word frequencies of simulated and real data on a log-log scale. The leftmost image plots word frequencies of simulated data sampled from the LDA-DGP with a symmetric $\boldsymbol\eta$. The light blue dashed and dotted line has a power law $\boldsymbol\alpha$ and the light green dashed line has a symmetric $\boldsymbol\alpha$. The center image plots word frequencies of simulated data sampled from the LDA-DGP with a power law $\boldsymbol\eta$. The green dashed and dotted line has a power law $\boldsymbol\alpha$ and the blue dashed line has a symmetric $\boldsymbol\alpha$. The rightmost image contains word frequencies from a random sample of 1,000 abstracts from grants awarded by the US National Institutes of Health (NIH) in 2014 [@nih], which is the solid orange line. Consistent with [@ha2002zipf], unigrams and bigrams are included. The dashed and dotted green line is the same as in the center image, provided as a comparison. All six plots have the same number of words.

The left and center images confirm the implications of the derrivation in Appendix 3. Specifically, that to generate word frequencies consistent with Zipf's law, $\boldsymbol\eta$ must have a power law distribution regardless of the shape of $\boldsymbol\alpha$. 

The rightmost image demonstrates that simulated data can have a power law word frequency distribution similar to real world data. But it also points to a calibration issue. The simulated data has a steeper slope, starting higher than the NIH data but dropping to zero faster. This indicates that the Zipf-Heaps relationship is different between the NIH data and the simulated data. I suspect that this is related to how I calibrated the slope of the power law in the simulation, not due to a structural impass. I leave this calibration to future research.

## Experiments

### Synthetic Data Generation and Evaluation Metrics

### Describing Properties of Synthetic Corpora

### Describing LDA Model Misspecification

## Discussion


## BACKUP
LDA is a latent variable model, as a result it can be challenging to study. We do not observe ground truth topics in real data, against which to compare the correctness of a topic model. Extrinsic evaluation method compare a topic model's results against a different ground truth---one that we do observe, such as a document's class. If a researcher's concern is document classification, then it is better to build a supervised classifier. LDA's strength is in its unsupervised nature, enabling us to discover that which we don't already know. It appears we have a "catch 22": we want to use LDA as a tool of discovery, but without ground truth, how do we know if our discovery is true or a statistical fluke? 

Fortunately, LDA models a data generating process, the LDA-DGP. Researchers can, and do, generate data sets by sampling from the LDA-DGP where they have chosen $K$, $\boldsymbol\alpha$, and $\boldsymbol\eta$ themselves. They then may use these simulated data sets to compare a model against a synthetic ground truth. Such "simulation studies" have a lengthy history in the statistical literature, going back to 1975 or further [@Hoaglin1975].

One cannot choose arbitrary parameters in the LDA-DGP and expect the results to reflect the statistical properties of language. In 2014, when conducting my own simulation study, I discovered that commonly used values for $\boldsymbol\eta$ cannot produce corpora consistent with Zipf's law [@zipf1949]. I include a proof of this in Appendix 2.[^phd] Figure 1 plots data simulated from the LDA-DGP against an actual corpus of NSF grant abstracts [@jones2014zipf]. The same holds for an asymmetric $\boldsymbol\eta$ if it is not proportional to a power law. The simulation corresponding to symmetric $\boldsymbol\eta$---by far the most common specification since Wallach et al. in 2009 [@wallach2009rethinking]---does not conform to a power law, which is linear in log-log space. Yet setting $\boldsymbol\eta$ proportional to a power law does produce power law distributed data, similar to the actual NSF abstracts data.[^pardon] Language has such stark statistical properties---i.e. power law distributions---that the validity of a simulation that cannot produce such properties is suspect.

[^phd]: Small as it was, this is the discovery that prompted me to seek a PhD. I wanted to complete this research, but knew that I could not do it without the structure and support of a formal program.

[^pardon]: Pardon the use of $\beta$ rather than $\eta$ in the figure. The figure is old and does not conform to my current notation scheme. I made the switch so that _tidylda_ would be more consistent with popular text analysis packages in the R ecosystem.

```{r nsf-corpus, fig.align = 'center', out.width = "100%", fig.cap = "Comparing two simulated data with different Dirichlet priors. The leftmost figure uses an asymmetric, but not proportional to a power law, parameter. The center figure uses a symmetric parameter. The rightmost figure compares simulations to word frequencies in an actual corpus of NSF grant abstracts. The simulation generated with a symmetric prior for words over topics---as is commonly used---is not consistent with Zipf's law. It represents an impossible prior. Only the simulation made with a prior proportional to a power law produces word frequencies similar to the actual corpus. From Jones and St. Thomas, 2014."}
knitr::include_graphics(here::here("figures", "nsf-sim2.png"))
```

Still, producing data with statistical properties of human language from the LDA-DGP means more than the low bar of one's simulation study not being invalid. If the LDA-DGP can produce data that shares the statistical properties of human language, then LDA is a valid model for analyzing corpora of human language. Put another way: when conducting a simulation study, Shi et al. state "our analysis is grounded on the assumption that a hidden topic structure exists in the texts" [@shi2019eval]. I go further: if the LDA-DGP can produce data consistent with statistical laws of language, then _this is the correct assumption to make_. 

Techniques---whether analytical of empirical---that discover the "right" model on simulated data can guide the researcher on real data sets. Yet it is unlikely that there is a "right" model for any real corpus. This is why I refer to "detecting pathologically misspecified models", rather than "finding the right model". It may also be that the LDA-DGP cannot perfectly reproduce relevant statistical laws of language. If that is the case, researchers must decide whether LDA is "good enough" or if a different topic model---for example CTMs [@blei2007ctm] or STMs [@roberts2013stm]---is a better choice. 

My objectives with this research are as follows: I wish to analytically link the LDA-DGP to relevant statistical laws of language[^beguninone], discover rules and heuristics from statistics on $\boldsymbol{X}$ to guide LDA model specification, and discover diagnostic statistics to discover whether an LDA model is pathologically misspecified. The remainder of this section is organized as follows: Section 4.1 summarizes related work from complex systems theory, stochastic simulation, and topic modeling. Section 4.2 outlines the approach I propose for this study.

[^beguninone]: I have begun this work with Appendix 2.



## Proposed Contributions

I propose making the following contributions:

1. Produce formal mathematical statements linking the LDA-DGP to Zipf's, Heaps's, and Taylor's laws of language as well as other corpus statistics such as correlation between words in two contexts,
2. Use principles of statistical design to plan and produce many synthetic corpora using the LDA-DGP that conform to statistical laws of human language and comprise a representative sample of the population of corpora that researchers may study with LDA,
3. Using (1) and (2) above, attempt to find rules or heuristics for specifying $K$, $\boldsymbol\alpha$, and $\boldsymbol\eta$ based on properties of the corpus, and
4. Using (1) and (2) above, diagnose the effects of model misspecification and attempt to develop diagnostic statistics or tests one may apply to an LDA model to detect pathological misspecification.

For (2), I intend to use principles from statistical design, recommended for such studies [@morrissimulationstudies]. Considerations for producing a collection of simulated data sets include varying observable corpus variables---the number of documents, document lengths, vocabulary sizes, correlations between documents, and possibly more---and varying latent LDA-DGP parameters---$K$, $\boldsymbol\alpha$ and $\boldsymbol\eta$. Another relationship that I may need to derive is the expected correlation between the words in any two documents produced by the LDA-DGP.[^covariance]

[^covariance]: The covariance between words in any two documents is a function of the interaction between the covariance given by independent draws from the Dirichlet distribution that produced the documents---$\boldsymbol\theta_d \sim \text{Dirichlet}(\boldsymbol\alpha)$---and the covariance given by independent draws from the Dirichlet distributions that produce the words---$\boldsymbol\beta_k \sim \text{Dirichlet}(\boldsymbol\eta)$.

For (3), I intend to consider the same variables. The goal is to use the observable corpus variables to predict the latent LDA-DGP parameters.

Diagnostic statistics that I am considering for (4) are: coherence metrics, likelihood metrics, $R^2$ (see below), and parameters from Zipf's, Heaps's, and Taylor's laws observed in the data compared to those predicted by drawing from the posterior of a fit LDA model. Pathological misspecifications I am considering are: too many or too few topics and misspecifications in shape or magnitude of $\boldsymbol\alpha$ or $\boldsymbol\eta$. I am also interested in exploring the effects of the procedure for optimizing $\boldsymbol\alpha$ [@minka2000estimating] employed in MALLET [@mallet].