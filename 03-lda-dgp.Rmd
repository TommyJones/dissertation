# Linking Zipf's law to the LDA-DGP {#ldadgp}

LDA is a latent variable model and, as a result, it can be challenging to study. No ground truth exists against which to compare models and methods. Selecting hyperparameters when building models is notoriously difficult as little formal guidance exists to guide a researcher's choices. Lack of ground truth complicates development of new metrics and methods. How is one to know if a new method is an improvement or a new metric insightful?

Fortuately, LDA models a generative process.This allows researchers to generate synthetic data sets by sampling from the LDA data generating process (LDA-DGP) where they have chosen $K$, $\boldsymbol\alpha$, and $\boldsymbol\eta$ themselves. They then may use these simulated data sets to compare a model against a synthetic ground truth. Such "simulation studies" have a lengthy history in the statistical literature, going back to 1975 or further [@Hoaglin1975]. Yet to be valid, synthetic data from the LDA-DGP must conform to empirical statistical laws of language such as Zipf's and Heaps's laws. 

In this chapter I examine the LDA-DGP analytically and empirically to do the following:

1. Link Zipf's law to the LDA-DGP,
2. Describe the roll of the magnitudes of $\boldsymbol\alpha$ and $\boldsymbol\eta$ in tuning the correlations between documents and topics, and
3. Describe the effects of misspecification of hyperparameters on the posterior of fit LDA models.

I find that [describe findings in brief here].

## Related Work

Work related to studying the LDA-DGP pulls from two seemingly disparate fields: topic modeling and complex systems theory. Complex systems theory deals in part with the emergence of power law distributions---as exemplified by some empirical laws of language---from complex systems [@cioffi2008power]. 

### Simulation Studies for LDA

Synthetic corpora appear commonly in topic modeling research. Shi et al. [@shi2019eval] list 14 works using synthetic corpora to study topic models. (See Table S3 in [@shi2019eval].) Most use some flavor of the LDA-DGP and focus on only one or two aspects of comparison between model and ground truth in the synthetic data set. Boyd-Graber, Hu, and Mimno suggest the use of semi-synthetic data---i.e., drawing simulations from the posterior---to capture properties of natural language [@boydgraber2017applications]. I am unaware of any work linking empirical laws of language to the LDA-DGP. Shi et al. link their simulation method to Zipf's law but it does not use the LDA-DGP to produce its simulations [@shi2019eval; @shi2019thesis]. 

Shi et al. go further than others and argue that use of synthetic corpora is "a principled approach" to evaluating topic models [@shi2019eval; @shi2019thesis]. Their approach does reproduce Zipf's law of language. Yet it does not use the LDA-DGP. It may represent a principled approach to studying probabilistic topic models with simulated data, but it is a parallel path to the one in this chapter. Using the LDA-DGP specifically allows for stronger statements related to pathological misspecification of LDA models. e.g., "Under the true model, then I would expect to see outcome A. Instead I see outcome B. Therefore, my model must be misspecified."[^linearanalogue] 

[^linearanalogue]: An analogue from linear regression might be, "under the true model, residuals are independent and identically distributed (_i.i.d._) Gaussian with mean zero. The residuals of my model are not _i.i.d._ Gaussian with mean zero. Therefore, my model must be misspecified."

### Empirical Language Laws

Altmann and Gerlach describe 9 universal laws purported to describe statistical regularities in human language [@altmann2015laws]. These laws are Zipf's [@zipf1949], Heaps's [@egghe2007untangling], Taylor's [@gerlach2014], Menzerath-Altmann [@altmann1980prolegomena], Recurrence [@zipf1949], Long-range correlation [@damerau1973tests], Entropy scaling [@altmann2015laws], Information content [@zipf1949], and various network topology laws [@altmann2015laws]. 

Of these laws, Zipf's and Heaps's laws are the most well known and relevant to LDA. The laws of Menzerath-Altmann, Recurrence, Long-range correlation, and Information content refer to properties of sub-words (e.g. length to information content), order of words, or proximity between words. LDA-DGP does not purport to model any of these. The law of Entropy scaling links entropy---in the information theoretic sense [@shannon2001mathematical]---to the number of words in a block of text. The LDA-DGP may or may not be able to reproduce this law. Similarly, some network views of a corpus may or may not apply to the LDA-DGP. Both may warrant future exploration but are less directly macroscopic properties of a corpus. 

Taylor's law may be relevant to the LDA-DGP but I leave its examination to future work as Zipf's and Heaps's laws are most commonly known as linguistic laws. Taylor's law was originally posed in the context of ecology [@taylor1961aggregation]. Its linguistic interpretation is that the standard deviation of the total number of words is proportional to the power of the mean of the total number of words in a corpus. Gerlach and Altmann explore the relationships between Zipf's, Heaps's, and Taylor's laws in the context of topic modeling [@gerlach2014]. Instead of using the LDA-DGP directly, they examine its asymptotic form, which is a Poisson process. 

#### Zipf's law
Zipf's law states that the frequency of a word is inversely proportional to the power of its frequency-rank. Zipf's law is not unique to any language as it appears to apply to all of them [@cancho2003]. Zipf's law has also been applied to the sizes of cities [@arshad2018zipf], casualties in armed conflict [@gillespie2015], and more. Zipf's law is a statement of a word's frequency and its rank. Yet if word frequencies in a corpus are plotted as a histogram, the power law relationship holds [@sole2008].

Empirical distributions of Zipf's law for large corpora demonstrate a relationship somewhat inconsistent with that predicted by Zipf's law. Some have proposed that the frequency-to-rank relationship is actually a set of broken power laws with one parametarization for the head of the distribution, another for the body, and a third parametarization for the tail. Yet, Ha et al. find that this is a trick of tokenization. "Language is not made of individual words but also consists of phrases of 2, 3 and more words, usually called n-grams for n=2, 3, etc." When including n-grams in both English and Chinese corpora, Ha et al. find that Zipf's law holds through the tail [@ha2002zipf]. Mandelbrot developed a generalization of Zipf's law that accounts for behavior at the head of the distribution [@mandelbrot1965information].

Formally, Zipf's law is

\begin{align}
F(r) \propto r^{-\gamma} \text{ for } \gamma \geq 1, r > 1
\end{align}

where $r$ is a word's rank, $F(r)$ is the frequency of a word's rank, and $\gamma$ is a parameter to be estimated. For human language $\gamma \approx 1$ [@cancho2003] and can be found through maximum likelihood estimation [@gillespie2015]. Altmann and Gerlach find $1.03 \leq \gamma \leq 1.58$ depending on the corpus and estimation method [@altmann2015laws]

Goldwater et al. explore the relationship between Zipf's law and then standard statistical models of language [@goldwater2011zipf]. They develop a framework for producing power law word frequencies in two stages. Critically, they link this framework to several models closely related to LDA but do not extend it to the LDA-DGP itself.

#### Heaps's law
Heaps's law states that the number of unique words in a corpus, $V$, scales sub-linearly with the total number of words in a corpus, $N$ [@heaps1978information]. Under mild assumptions, Heaps's law is asymptotically equivalent to Zipf's law [@kornai1999zipf]. Unlike Zipf's law, it is an increasing power law.

\begin{align}
V \propto N^\delta \text{ for } N > 1, 0 < \delta < 1
\end{align}

There are several ways to compute Heaps's law for a single corpus. Altmann and Gerlach use two methods: the first computes $V$ and $N$ for each document in the corpus and the second progresses over each word in a corpus calculating new values of $V$ and $N$ as each word is added. The latter approach works for single documents of sufficient length as well, such as a book [@altmann2015laws]. 

## The LDA Data Generating Process (LDA-DGP)

Assuming there are $D$ contexts, $K$ topics, $V$ unique words, $N$ total words and $N_d$ words in the $d$-th context, the LDA-DGP is as follows. For each word, $n$, in context $d$: 

1. Generate $\boldsymbol{B}$ by sampling $K$ topics $\boldsymbol\beta_k \sim \text{Dirichlet}(\boldsymbol\eta), \forall k \in \{1,2,...,K\}$
2. Generate $\boldsymbol\Theta$ by sampling $D$ documents $\boldsymbol\theta_d \sim \text{Dirichlet}(\boldsymbol\alpha), \forall d \in \{1,2, ..., D\}$
3. Then for each context, $d$
1. Draw topic $z_{d,n}$ from $\text{Multinomial}(\boldsymbol\theta_d)$
2. Draw word $w_{d,n}$ from $\text{Multinomial}(\boldsymbol\beta_{z_{d,n}})$
3. Repeat 1. and 2. $N_d$ times.

Note that context $d$ has $N_d$ words $\forall d \in \{1, 2, ..., D\}$ and that the total number of words in the corpus is $N = \sum_{d=1}^D N_d$.

## Linking the LDA-DGP to Zipf's law
Zipf's law describes the relative frequencies between words in a corpus. Therefore, the expected term frequency distribution of a corpus drawn from the LDA-DGP should describe the relationship between Zipf's law and the LDA-DGP. The following relationship is derived in Appendix 3:

\begin{align}
\mathbb{E}(w_v) &=
N \frac{\eta_v}{\sum_{v = 1}^V \eta_v} \label{eq:zipf}
\end{align}

where $w_v$ is the total number of times word $v$ was sampled across all $D$ contexts.

\@ref(eq:zipf) implies that the relative frequencies of words $i$ and $j$ are given by their ranks in a power law relationship. In other words, the shape of $\boldsymbol\eta$ is a power law, given by Zipf's law. No elements of $\boldsymbol\alpha$ appear in \@ref(eq:zipf). As shown in Appendix 3, the $\alpha_i$'s sum and factor out of the derivation. This makes some intuitive sense. An author can theoretically write an entire corpus of documents about only one topic or an author may write a corpus of documents with an even distribution of topics. In either case the aggregate word frequencies must follow Zipf's law.

This result is consistent with the framework laid out by Goldwater et al. [@goldwater2011zipf]. The Dirichlet multinomial model - of which LDA is an application - is a special case of their framework. In this framework, the Dirichlet prior has a parameter following a power law. They did not put this in the specific form of LDA, however, as is done in this paper. Using this specification of the Dirichlet multinomial it is now possible to generate corpora with the statistical properties of natural language within an LDA framework. This facilitates exploring the properties of LDA under different conditions and the development of metrics aiding in model specification and post-estimation diagnostics. 

The relationship in \@ref(eq:zipf) also implies that one can estimate the shape of $\boldsymbol\eta$ from a document term matrix. Formally linking Zipf's law with the LDA-DGP we have

\begin{align}
    \mathbb{E}(w_v) &=
        N \frac{\eta_v}{\sum_{v = 1} ^ V eta_v} =
        C \cdot r_v ^{-\gamma}
\end{align}

$C$ and $\gamma$ may be estimated from data and $r_v$, the rank of word $v$, is known. Some algebra then yields

\begin{align}
    \frac{\eta_v}{\sum_{v = 1}^V \eta_v} = \frac{C}{N}\cdot r_v ^ {-\gamma}, \forall v
\end{align}

What still remains is for a researcher to specify the magnitude of $\boldsymbol\eta$, in other words $\sum_{v=1}^V \eta_v$, and of course $\boldsymbol\alpha$ and the number of topics, $K$. The empirical evidence for specifying $\boldsymbol\eta$ as a power law rather than a flat prior is explored, though not resoved, in section 3.4.3.

```{r compare-hypers, fig.cap = "this is a caption", fig.width = 7, fig.height = 2.5}

library(tidyverse)
library(patchwork)

nih_plotmat <- read_rds("data-derived/zipf-analysis/nih-plotmat.rds")

p1 <- nih_plotmat |> ggplot() + 
  geom_line(aes(x = rank, y = pl_pl), color = NA, lty = 4, lwd = 1.25) + 
  geom_line(aes(x = rank, y = npl_npl), color = "#b2df8a", lty = 2, lwd = 1.25) +
  geom_line(aes(x = rank, y = pl_npl), color = "#a6cee3", lty = 4, lwd = 1.25) + 
  ylim(1, 10000) +
  scale_x_continuous(trans='log10') +
  scale_y_continuous(trans='log10') +
  ylab("Frequency") +
  xlab("Rank") +
  ggtitle("Symmetric Eta")

p2 <- nih_plotmat |> ggplot() + 
  geom_line(aes(x = rank, y = pl_pl), color = "#1f78b4", lty = 4, lwd = 1.25) + 
  geom_line(aes(x = rank, y = npl_pl), color = "#33a02c", lty = 2, lwd = 1.25) + 
  ylim(1, 10000) + 
  scale_x_continuous(trans='log10') +
  scale_y_continuous(trans='log10') +
  ylab("") +
  xlab("Rank") + 
  ggtitle("Power Law Eta")

p3 <- nih_plotmat |> ggplot() + 
  geom_line(aes(x = rank, y = nih), color = "#d95f02", lwd = 1.25) + 
  geom_line(aes(x = rank, y = pl_pl), color = "#1b9e77", lty = 4, lwd = 1.25) + 
  ylim(1, 10000) +
  scale_x_continuous(trans='log10') +
  scale_y_continuous(trans='log10') +
  ylab("") +
  xlab("Rank") + 
  ggtitle("NIH Corpus")


plot(p1 + p2 + p3)

```

Figure \@ref(fig:compare-hypers) plots word frequencies of simulated and real data on a log-log scale. The leftmost image plots word frequencies of simulated data sampled from the LDA-DGP with a symmetric $\boldsymbol\eta$. The light blue dashed and dotted line has a power law $\boldsymbol\alpha$ and the light green dashed line has a symmetric $\boldsymbol\alpha$. The center image plots word frequencies of simulated data sampled from the LDA-DGP with a power law $\boldsymbol\eta$. The green dashed and dotted line has a power law $\boldsymbol\alpha$ and the blue dashed line has a symmetric $\boldsymbol\alpha$. The rightmost image contains word frequencies from a random sample of 1,000 abstracts from grants awarded by the US National Institutes of Health (NIH) in 2014 [@nih], which is the solid orange line. Consistent with [@ha2002zipf], unigrams and bigrams are included. Crucially, stopwords have _not_ been removed from the NIH data, allowing the head of the distribution to reach its full magnitude. The dashed and dotted green line is the same as in the center image, provided as a comparison. All six plots have the same number of contexts and words. The left and center images confirm the implications of the derrivation in Appendix 3. Specifically, that to generate word frequencies consistent with Zipf's law, $\boldsymbol\eta$ must have a power law distribution regardless of the shape of $\boldsymbol\alpha$. The rightmost image demonstrates that simulated data can have a power law word frequency distribution similar to real world data.

Finally, the asymptotic equivalence between Zipf's and Heap's law indicates that $\boldsymbol\eta$ drives both Zipf's and Heap's laws, the two most commonly-understood emprical language laws.

## Experiments
Simulation studies allow researchers to know the true parameters that generated a data set and evaluate how an estimated model performs against "the truth". But if simulated data cannot replicate the key statistical properties of its real world counterpart, it calls into question any conclusion made with such synthetic data. However, linking the LDA-DGP Zipf's law (and Heaps's law) adds rigor to simulation studies of LDA by giving synthetic data the gross statistical properties of human language. 

In this section, I generate 4,096 synthetic data sets using different parametarizations of the LDA-DGP and baseline corpus parameters, such as number of contexts, vocabulary size, etc. Each of these corpora are consistent with Zipf's/Heaps's laws. After briefly describing the properties of these synthetic corpora, I use them to explore the effects of misspecifying hyperparameters when fitting LDA models.

### Experimental Set Up

#### Synthetic Data Generation
Each data set is drawn from a model with a unique combination of the following hyper parameters, resulting in 4,096 unique combinations:

1. $\sum_{v=1} ^ V \eta_v \in \{50, 250, 500, 1000\}$
2. $\sum_{k = 1} ^ K \alpha_k \in \{0.5, 1, 3, 5\}$
3. $N_d \sim \text{Poi}(\lambda)$ where $\lambda \in \{50, 100, 200, 400\}$
4. $V \in \{1000, 5000, 10000, 20000\}$
5. $D \in \{500, 1000, 2000, 4000\}$
6. $K \in \{25, 50, 100, 200\}$

I choose $\boldsymbol\eta$ to be proportional to a power law, consistent with Appendix 3 and Zipf's law [@zipf1949]. I chose $\boldsymbol\alpha$ to be symmetric for all data sets. The magnitudes of the Dirichlet hyperparameters, $\boldsymbol\eta$ and $\boldsymbol\alpha$ vary while keeping their shapes fixed. The magnitude of the parameter of a Dirichlet distribution plays a strong role in tuning the covariance between draws from that distribution. A smaller magnitude means larger variability between draws. In other words, if $\left(\sum_{v=1}^V \eta_v \right)$ is smaller, topics are less linguistically similar. If $\left(\sum_{v=1}^V \eta_v \right)$ is larger, topics are more linguistically similar. Similarly, $\left(\sum_{k=1}^K \alpha_k \right)$ tunes the topical similarity of documents.

#### Modeling Choices

For each[^tookasample] of the simulated data sets I fit five LDA models, one perfectly specified according to the parameters that generated the data and four misspecified models. The misspecified models misspecify $K$, $\sum_{k = 1}^K \alpha_k$, $\sum_{v = 1}^V \eta_v$, and the shape of $\boldsymbol\eta$, making it flat. For each model, the Gibbs sampler was run for 200 iterations, 150 of which were burn in iterations. The final posterior is the average over the last 50 iterations.

[^tookasample]: As of the initial draft of this work, completed on approximately 10/3/2022, I took a random sample of 1,024 data sets to perform the modeling analysis. I have written up these preliminary results. I will perform the analysis on the full set of 4,096 data sets in the final version of this dissertation.

#### Evaluation Metrics

For each synthetic data set I calculate parameter values for Zipf's law using linear regression in log-log space. Specifically, I fit $\log_{10} w_v = \log_{10}C -\gamma\log_{10}r_v + \epsilon_v$, where $\epsilon_v$ is the linear regression error term for the $v$-th word. This method is often biased by low rank words. To compensate, I use the `poweRLaw` package [@rpowerlawjss] to select a threshold for excluding low rank words. 

I also calculate several statistics from the synthetic data that a researcher would have available to them. These are $N$, the total number of word instances, $\hat{V}$, the total *observed* vocabulary size, and their ratio, $\frac{N}{\hat{V}}$. 

When fitting LDA models using Gibbs sampline, it is useful to see if the chain has converged. To do this, I use Geweke's convergence statistic [@geweke1991evaluating] on the log likelihood of the estimated model over the last 50 iterations of the chain (the area over which I average the posteriors). The test statistic is a standard Z-score on the difference between two sample means of two partitions of the chain, generally the first 10 percent and last 50 percent. If the two partitions are not significantly different, one can assume that the chain has converged. I compare the absolute values of the Geweke test statistic between the perfectly specified and misspecified models to estimate the degree to which model misspecification affects convergence. The absolute value is used because the Geweke test statistic is drawn from a symmetric two-tailed distribution where extremes have both positive and negative values. 

In addition to the Geweke statistic, I also compare the following between correctly-specified and misspecified models:

* Log Likelihood, averaged over the last 50 Gibbs iterations, of perfectly specified to misspecified models. The log likelihood calculation follows that used in [@chen2015warplda].
* Coefficient of determination, or $R^2$. This statistic calculates the proportion of variability in the data that has been accounted for in the fit model. This statistic and a study of its properties is the next chapter of this dissertation.
* Probabilistic coherence (see Appendix [5])---or just "coherence"---averaged over all topics in a model. 
* Prevalence of each topic, averaged across all topics in the model. The prevalence of topic $k$ is calculated with $\sum_{d = 1} ^ D N_d \cdot \theta_{d,k}$.

In the comparing correctly specified and misspecified models, I use a paired t-statistic to calculate 95% confidence intervals in the difference between the correct (i.e., "untreated") and misspecified (i.e., "treated") groups. This paired t-statistic confidence interval is applied to the Geweke statistics, log likelihood, $R^2$, probabilistic coherence, and prevalence.

### Describing Properties of Synthetic Corpora

### Describing LDA Model Misspecification

```{r model-plots, fig.cap = "this is my caption", fig.width = 7, fig.height = 5}
t_tests <- read_rds("data-derived/zipf-analysis/model-t-tests.rds")

times_help <- function(x, y) x * y

ll_plot <- 
  t_tests |> 
  filter(outcome == "ll") |> 
  ggplot() + 
  geom_bar(aes(x = parameter, y = mean_diff, fill = factor(parameter)), stat = "identity") + 
  geom_errorbar(aes(x = parameter, ymin = ci_95_low, ymax = ci_95_high)) + 
  geom_hline(yintercept = 0, lty = 2) +
  xlab("") + ylab("") +
  ggtitle("Log Likelihood") +
  theme(legend.position = "none") +
  theme(axis.text.x = element_blank()) +
  ylim(
    c(
      t_tests$ci_95_low[t_tests$outcome == "ll"], 
      t_tests$ci_95_high[t_tests$outcome == "ll"]
    ) |> 
      abs() |> max() %>% times_help(-1),
    c(
      t_tests$ci_95_low[t_tests$outcome == "ll"], 
      t_tests$ci_95_high[t_tests$outcome == "ll"]
    ) |> 
      abs() |> max()
  )

r2_plot <- 
  t_tests |> 
  filter(outcome == "r2") |> 
  ggplot() + 
  geom_bar(aes(x = parameter, y = mean_diff, fill = factor(parameter)), stat = "identity") + 
  geom_errorbar(aes(x = parameter, ymin = ci_95_low, ymax = ci_95_high)) + 
  geom_hline(yintercept = 0, lty = 2) +
  xlab("") + ylab("") +
  ggtitle("R-Squared") +
  theme(legend.position = "none") +
  theme(axis.text.x = element_blank()) +
  ylim(
    c(
      t_tests$ci_95_low[t_tests$outcome == "r2"], 
      t_tests$ci_95_high[t_tests$outcome == "r2"]
    ) |> 
      abs() |> max() %>% times_help(-1),
    c(
      t_tests$ci_95_low[t_tests$outcome == "r2"], 
      t_tests$ci_95_high[t_tests$outcome == "r2"]
    ) |> 
      abs() |> max()
  )

mean_coherence_plot <- 
  t_tests |> 
  filter(outcome == "mean_coherence") |> 
  ggplot() + 
  geom_bar(aes(x = parameter, y = mean_diff, fill = factor(parameter)), stat = "identity") + 
  geom_errorbar(aes(x = parameter, ymin = ci_95_low, ymax = ci_95_high)) + 
  geom_hline(yintercept = 0, lty = 2) +
  xlab("") + ylab("") +
  ggtitle("Mean Coherence") +
  theme(legend.position = "none") +
  theme(axis.text.x = element_text(angle=45, hjust=1)) +
  ylim(
    c(
      t_tests$ci_95_low[t_tests$outcome == "mean_coherence"], 
      t_tests$ci_95_high[t_tests$outcome == "mean_coherence"]
    ) |> 
      abs() |> max() %>% times_help(-1),
    c(
      t_tests$ci_95_low[t_tests$outcome == "mean_coherence"], 
      t_tests$ci_95_high[t_tests$outcome == "mean_coherence"]
    ) |> 
      abs() |> max()
  )

mean_prevalence_plot <- 
  t_tests |> 
  filter(outcome == "mean_prevalence") |> 
  ggplot() + 
  geom_bar(aes(x = parameter, y = mean_diff, fill = factor(parameter)), stat = "identity") + 
  geom_errorbar(aes(x = parameter, ymin = ci_95_low, ymax = ci_95_high)) + 
  geom_hline(yintercept = 0, lty = 2) +
  xlab("") + ylab("") +
  ggtitle("Mean Prevalence") +
  theme(legend.position = "none") +
  theme(axis.text.x = element_text(angle=45, hjust=1)) +
  ylim(
    c(
      t_tests$ci_95_low[t_tests$outcome == "mean_prevalence"], 
      t_tests$ci_95_high[t_tests$outcome == "mean_prevalence"]
    ) |> 
      abs() |> max() %>% times_help(-1),
    c(
      t_tests$ci_95_low[t_tests$outcome == "mean_prevalence"], 
      t_tests$ci_95_high[t_tests$outcome == "mean_prevalence"]
    ) |> 
      abs() |> max()
  )

geweke_plot <- 
  t_tests |> 
  filter(outcome == "geweke") |> 
  ggplot() + 
  geom_bar(aes(x = parameter, y = mean_diff, fill = factor(parameter)), stat = "identity") + 
  geom_errorbar(aes(x = parameter, ymin = ci_95_low, ymax = ci_95_high)) + 
  geom_hline(yintercept = 0, lty = 2) +
  xlab("") + ylab("") +
  ggtitle("Geweke Statistic") +
  theme(legend.position = "none") +
  theme(axis.text.x = element_text(angle=45, hjust=1)) +
  ylim(
    c(
      t_tests$ci_95_low[t_tests$outcome == "geweke"], 
      t_tests$ci_95_high[t_tests$outcome == "geweke"]
    ) |> 
      abs() |> max() %>% times_help(-1),
    c(
      t_tests$ci_95_low[t_tests$outcome == "geweke"], 
      t_tests$ci_95_high[t_tests$outcome == "geweke"]
    ) |> 
      abs() |> max()
  )

pw <- 
  (ll_plot + r2_plot + plot_spacer() + 
  mean_coherence_plot + mean_prevalence_plot + geweke_plot) +
  plot_annotation(title = "Mean Differences") 

plot(pw)

```


## Discussion


## BACKUP
LDA is a latent variable model, as a result it can be challenging to study. We do not observe ground truth topics in real data, against which to compare the correctness of a topic model. Extrinsic evaluation method compare a topic model's results against a different ground truth---one that we do observe, such as a document's class. If a researcher's concern is document classification, then it is better to build a supervised classifier. LDA's strength is in its unsupervised nature, enabling us to discover that which we don't already know. It appears we have a "catch 22": we want to use LDA as a tool of discovery, but without ground truth, how do we know if our discovery is true or a statistical fluke? 

Fortunately, LDA models a data generating process, the LDA-DGP. Researchers can, and do, generate data sets by sampling from the LDA-DGP where they have chosen $K$, $\boldsymbol\alpha$, and $\boldsymbol\eta$ themselves. They then may use these simulated data sets to compare a model against a synthetic ground truth. Such "simulation studies" have a lengthy history in the statistical literature, going back to 1975 or further [@Hoaglin1975].

One cannot choose arbitrary parameters in the LDA-DGP and expect the results to reflect the statistical properties of language. In 2014, when conducting my own simulation study, I discovered that commonly used values for $\boldsymbol\eta$ cannot produce corpora consistent with Zipf's law [@zipf1949]. I include a proof of this in Appendix 2.[^phd] Figure 1 plots data simulated from the LDA-DGP against an actual corpus of NSF grant abstracts [@jones2014zipf]. The same holds for an asymmetric $\boldsymbol\eta$ if it is not proportional to a power law. The simulation corresponding to symmetric $\boldsymbol\eta$---by far the most common specification since Wallach et al. in 2009 [@wallach2009rethinking]---does not conform to a power law, which is linear in log-log space. Yet setting $\boldsymbol\eta$ proportional to a power law does produce power law distributed data, similar to the actual NSF abstracts data.[^pardon] Language has such stark statistical properties---i.e. power law distributions---that the validity of a simulation that cannot produce such properties is suspect.

[^phd]: Small as it was, this is the discovery that prompted me to seek a PhD. I wanted to complete this research, but knew that I could not do it without the structure and support of a formal program.

[^pardon]: Pardon the use of $\beta$ rather than $\eta$ in the figure. The figure is old and does not conform to my current notation scheme. I made the switch so that _tidylda_ would be more consistent with popular text analysis packages in the R ecosystem.

```{r nsf-corpus, fig.align = 'center', out.width = "100%", fig.cap = "Comparing two simulated data with different Dirichlet priors. The leftmost figure uses an asymmetric, but not proportional to a power law, parameter. The center figure uses a symmetric parameter. The rightmost figure compares simulations to word frequencies in an actual corpus of NSF grant abstracts. The simulation generated with a symmetric prior for words over topics---as is commonly used---is not consistent with Zipf's law. It represents an impossible prior. Only the simulation made with a prior proportional to a power law produces word frequencies similar to the actual corpus. From Jones and St. Thomas, 2014."}
knitr::include_graphics(here::here("figures", "nsf-sim2.png"))
```

Still, producing data with statistical properties of human language from the LDA-DGP means more than the low bar of one's simulation study not being invalid. If the LDA-DGP can produce data that shares the statistical properties of human language, then LDA is a valid model for analyzing corpora of human language. Put another way: when conducting a simulation study, Shi et al. state "our analysis is grounded on the assumption that a hidden topic structure exists in the texts" [@shi2019eval]. I go further: if the LDA-DGP can produce data consistent with statistical laws of language, then _this is the correct assumption to make_. 

Techniques---whether analytical of empirical---that discover the "right" model on simulated data can guide the researcher on real data sets. Yet it is unlikely that there is a "right" model for any real corpus. This is why I refer to "detecting pathologically misspecified models", rather than "finding the right model". It may also be that the LDA-DGP cannot perfectly reproduce relevant statistical laws of language. If that is the case, researchers must decide whether LDA is "good enough" or if a different topic model---for example CTMs [@blei2007ctm] or STMs [@roberts2013stm]---is a better choice. 

My objectives with this research are as follows: I wish to analytically link the LDA-DGP to relevant statistical laws of language[^beguninone], discover rules and heuristics from statistics on $\boldsymbol{X}$ to guide LDA model specification, and discover diagnostic statistics to discover whether an LDA model is pathologically misspecified. The remainder of this section is organized as follows: Section 4.1 summarizes related work from complex systems theory, stochastic simulation, and topic modeling. Section 4.2 outlines the approach I propose for this study.

[^beguninone]: I have begun this work with Appendix 2.



## Proposed Contributions

I propose making the following contributions:

1. Produce formal mathematical statements linking the LDA-DGP to Zipf's, Heaps's, and Taylor's laws of language as well as other corpus statistics such as correlation between words in two contexts,
2. Use principles of statistical design to plan and produce many synthetic corpora using the LDA-DGP that conform to statistical laws of human language and comprise a representative sample of the population of corpora that researchers may study with LDA,
3. Using (1) and (2) above, attempt to find rules or heuristics for specifying $K$, $\boldsymbol\alpha$, and $\boldsymbol\eta$ based on properties of the corpus, and
4. Using (1) and (2) above, diagnose the effects of model misspecification and attempt to develop diagnostic statistics or tests one may apply to an LDA model to detect pathological misspecification.

For (2), I intend to use principles from statistical design, recommended for such studies [@morrissimulationstudies]. Considerations for producing a collection of simulated data sets include varying observable corpus variables---the number of documents, document lengths, vocabulary sizes, correlations between documents, and possibly more---and varying latent LDA-DGP parameters---$K$, $\boldsymbol\alpha$ and $\boldsymbol\eta$. Another relationship that I may need to derive is the expected correlation between the words in any two documents produced by the LDA-DGP.[^covariance]

[^covariance]: The covariance between words in any two documents is a function of the interaction between the covariance given by independent draws from the Dirichlet distribution that produced the documents---$\boldsymbol\theta_d \sim \text{Dirichlet}(\boldsymbol\alpha)$---and the covariance given by independent draws from the Dirichlet distributions that produce the words---$\boldsymbol\beta_k \sim \text{Dirichlet}(\boldsymbol\eta)$.

For (3), I intend to consider the same variables. The goal is to use the observable corpus variables to predict the latent LDA-DGP parameters.

Diagnostic statistics that I am considering for (4) are: coherence metrics, likelihood metrics, $R^2$ (see below), and parameters from Zipf's, Heaps's, and Taylor's laws observed in the data compared to those predicted by drawing from the posterior of a fit LDA model. Pathological misspecifications I am considering are: too many or too few topics and misspecifications in shape or magnitude of $\boldsymbol\alpha$ or $\boldsymbol\eta$. I am also interested in exploring the effects of the procedure for optimizing $\boldsymbol\alpha$ [@minka2000estimating] employed in MALLET [@mallet].