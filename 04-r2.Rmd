# Coefficient of Determination for Topic Models {#r2}

According to an often-quoted but never cited definition, "the goodness of fit of a statistical model describes how well it fits a set of observations. Measures of goodness of fit typically summarize the discrepancy between observed values and the values expected under the model in question."[^goodness] Goodness of fit measures vary with the goals of those constructing the statistical model. Inferential goals may emphasize in-sample fit while predictive goals may emphasize out-of-sample fit. Prior information may be included in the goodness of fit measure for Bayesian models, or it may not. Goodness of fit measures may include methods to correct for model over fitting. In short, goodness of fit measures the performance of a statistical model against the ground truth of observed data. Fitting the data well is generally a necessary---though not sufficient---condition for trust in a statistical model, whatever its goals.

[^goodness]: This quote appears verbatim on Wikipedia and countless books, papers, and websites. I cannot find its original source.

Many researchers have eschewed goodness of fit measures as metrics of evaluating topic models. This is unfortunate. Goodness of fit is often the first line of defense against a pathologically misspecified model. If the model does not fit the data well, can it be relied on for inference or interpretation? Of course goodness of fit is not the only, perhaps not even the most important, measure of a good model. In fact a model that fits its training data too well is itself problematic. Nevertheless, a consistent and easily-interpreted measure of goodness of fit for topic models can serve to demystify the dark art of topic modeling to otherwise statistically literate audiences. 

Goodness of fit manifests itself in topic modeling through word frequencies. Topic models are not fully unsupervised methods. If they were unsupervised, this would mean that no observations exist upon which to compare a model’s fitted values. However, probabilistic topic models are ultimately generative models of word frequencies [@blei2002lda]. The expected value of word frequencies in a document under a topic model is given by the expected value of a multinomial random variable. The that can be compared to the predictions, then, are the word frequencies themselves. Most goodness of fit measures in topic modeling are restricted to in-sample fit. Yet some out-of-sample measures have been developed [@buntine2009likelihood].

For any probabilistic topic model, the following relationship holds

\begin{align}
  \mathbb{E}(\boldsymbol{X}) &= \boldsymbol{n} \odot \boldsymbol\Theta \cdot \boldsymbol{B}
\end{align}

Above, $\boldsymbol{n}$ is a $D$-length vector whose $d$-th entry is the number of terms in the $d$-th document and $\odot$ denotes elementwise multiplication.

## Related Work

Evaluation methods for topic models can be broken down into three categories: manual inspection, intrinsic evaluation, and extrinsic evaluation [@shi2019eval]. Manual inspection involves human judgement upon examining the outputs of a topic model. Intrinsic evaluation measures performance based on model internals and its relation to training data. R-squared is an intrinsic evaluation method. Extrinsic methods compare model outputs to external information not explicitly modeled, such as document class. 

Manual inspection is subjective but closely tracks how many topic models are used in real-world applications. Most research on topic model evaluation has focused on presenting ordered lists of words that meet human judgement about words that belong together. For each topic, words are ordered from the highest value of $\boldsymbol\beta_k$ to the lowest (i.e. the most to least frequent in each topic). In [@chang2009tea] the authors introduce the "intruder test." Judges are shown a few high-probability words in a topic, with one low-probability word mixed in. Judges must find the low-probability word, the intruder. They then repeat the procedure with documents instead of words. A good topic model should allow judges to easily detect the intruders.

One class of intrinsic evaluation methods attempt to approximate human judgment. These metrics are called "coherence" metrics. Coherence metrics attempt to approximate the results of intruder tests in an automated fashion. Researchers have put forward several coherence measures. These typically compare pairs of highly-ranked words within topics. Röder et al. evaluate several of these [@roder2015coherence]. They have human evaluators rank topics by quality and then compare rankings based on various coherence measures to the ranking of the evaluators. They express skepticism that existing coherence measures are sufficient to assess topic quality. In a paper presented at the conference of the Association for Computational Linguistics (ACL), [@lau2014machine] find that normalized pointwise mutual information (NPMI) is a coherence metric that closely resembles human judgement. 

Other popular intrinsic methods are types of goodness of fit. The primary goodness of fit measures in topic modeling are likelihood metrics. Likelihoods, generally the log likelihood, are naturally obtained from probabilistic topic models. Likelihoods may contain prior information, as is often the case with Bayesian models. If prior information is unknown or undesired, researchers may calculate the likelihood using only estimated parameters. Researchers have used likelihoods to select the number of topics [@griffiths2004scientific], compare priors [@wallach2009rethinking], or otherwise evaluate the efficacy of different modeling procedures [@asuncion2012smoothing] [@nguyen2014sometimes]. A popular likelihood method for evaluating out-of-sample fit is called perplexity. Perplexity measures a transformation of the likelihood of the held-out words conditioned on the trained model.

The most common extrinsic evaluation method is to compare topic distributions to known document classes. The most prevalent topic in each document is taken as a document’s topical classification. Then, researchers will calculate precision, recall, and/or other diagnostic statistics for classification as measures of a topic model's success.

Though useful, prevalent evaluation metrics in topic modeling are difficult to interpret, are inappropriate for use in topic modeling, or cannot be produced easily. Intruder tests are time-consuming and costly, making intruder tests infeasible to conduct regularly. Coherence is not primarily a goodness of fit measure. AUC, precision, and recall metrics mis-represent topic models as binary classifiers. This misrepresentation ignores one fundamental motivation for using topic models: allowing documents to contain multiple topics. This approach also requires substantial subjective judgement. Researchers must examine the high-probability words in a topic and decide whether it corresponds to the corpus topic tags or not.

Likelihoods have an intuitive definition: they represent the probability of observing the training data if the model is true. Yet properties of the underlying corpus influence the scale of the likelihood function. Adding more documents, having a larger vocabulary, and even having longer documents all reduce the likelihood. Likelihoods of multiple models on the same corpus can be compared. (Researchers often do this to help select the number of topics for a final model [@griffiths2004scientific].) Topic models on different corpora cannot be compared, however.[^actually] One corpus may have 1,000 documents and 5,000 tokens, while another may have 10,000 documents and 25,000 tokens. The likelihood of a model on the latter corpus will be much smaller than a model on the former. Yet this does not indicate the model on the latter corpus is a worse fit; the likelihood function is simply on a different scale. Perplexity is a transformation of the likelihood often used for out-of-sample documents. The transformation makes the interpretation of perplexity less intuitive than a raw likelihood. Further, perplexity’s scale is influenced by the same factors as the likelihood.

[^actually]: Actually, I am cautiously hopeful that my work in transfer learning can enable such comparisons based on changes of the same topic from the same base model fine tuned to two different corpora. The scale issue related to comparison via log likelihood will still remain, however.

## The Coefficient of Determination: $R^2$

The coefficient of determination is a popular, intuitive, and easily-interpretable goodness of fit measure. The coefficient of determination, denoted $R^2$, is most common in ordinary least squares (OLS) regression. However, researchers have developed $R^2$ and several pseudo $R^2$ measures for many classes of statistical models. The largest value of $R^2$ is 1, indicating a model fits the data perfectly. The formal definition of $R^2$ (below) is interpreted---without loss of generality---as the proportion of _variability_ in the data that is explained by the model. For linear models with outcomes in $\mathbb{R}_1$, $R^2$ is bound between 0 and 1 and is the proportion of _variance_ in the data explained by the model [@neter1996applied]. Even outside of the context of a linear model, $R^2$ retains its maximum of 1 and its interpretation as the proportion of explained variability. Negative values of $R^2$ are possible for non-linear models or models in $\mathbb{R}_M$ where $M > 1$. These negative values indicate that simply guessing the mean outcome is a better fit than the model.

### The Standard Definition of $R^2$

For a model, $f$, of outcome variable, $y$, where there are $N$ observations, $R^2$ is derived from the following:

\begin{align}
  \bar{y} &= \frac{1}{N}\sum_{i=1}^{N}y_i\\
  SS_{tot.} &= \sum_{i=1}^N{(y_i-\bar{y})^2}\\
  SS_{resid.} &= \sum_{i=1}^N{(f_i-y_i)^2}
\end{align}

Thus, the standard definition of $R^2$ is a ratio of summed squared errors. 

\begin{align}
    R^2 \equiv 1 - \frac{SS_{resid.}}{SS_{tot.}}
\end{align} 

### A Geometric Interpretation of $R^2$

$R^2$ has a geometric interpretation as well. $SS_{tot.}$ is the total squared-Euclidean distance from each $y_i$ to the mean outcome, $\bar{y}$. Then $SS_{resid.}$ is the total squared-Euclidean distance from each $y_i$ to its predicted value under the model, $f_i$. Recall that for any two points $\boldsymbol{p}, \boldsymbol{q} \in \mathbb{R}_M$

\begin{align}
	d(\boldsymbol{p},\boldsymbol{q}) = \sqrt{\sum_{i=1}^M{(p_i - q_i)^2}}
\end{align}

where $d(\boldsymbol{p}, \boldsymbol{q})$ denotes the Euclidean distance between $\boldsymbol{p}$ and $\boldsymbol{q}$. $R^2$ is often taught in the context of OLS where $y_i, f_i \in \mathbb{R}_1$. In that case, $d(y_i, f_i) = \sqrt{(y_i - f_i)^2}$; by extension $d(y_i, \bar{y}) = \sqrt{(y_i - \bar{y})^2}$. In the multidimensional case where $\boldsymbol{y}_i, \boldsymbol{f}_i \in \mathbb{R}_M; M > 1$, then $\bar{\boldsymbol{y}} \in \mathbb{R}_M$ represents the point at the center of the data in $\mathbb{R}_M$.[^r2oneD]

[^r2oneD]: In the one-dimensional case, $y_i , f_i \in \mathbb{R}_1$, $SS_{resid.}$ can be considered the squared-Euclidean distance between the $n$-dimensional vectors $y$ and $f$. However, this relationship does not hold when $\boldsymbol{y}_i , \boldsymbol{f}_i \in \mathbb{R}_M ; M > 1$.


We can rewrite $R^2$ using the relationships above. Note than now $\bar{\boldsymbol{y}}$ is now a vector with $M$ entries. The $j$-th entry of $\bar{\boldsymbol{y}}$ is averaged across all $N$ vectors. i.e $\bar{y}_j = \frac{1}{N} \sum_{i=1}^{N} y_{i,v}$. From there we have:

\begin{align}
    \bar{\boldsymbol{y}} &= \frac{1}{N} \sum_{i=1}^{N} \boldsymbol{y}_i \\ 
    SS_{tot.} &= \sum_{i=1}^N{d(\boldsymbol{y}_i, \bar{\boldsymbol{y}}})^2\\
    SS_{resid.} &= \sum_{i=1}^N{d(\boldsymbol{y}_i, \boldsymbol{f}_i)^2}\\
    \Rightarrow R^2 & \equiv 1 - \frac{SS_{resid.}}{SS_{tot.}}
\end{align}

Fig. [X] visualizes the geometric interpretation of $R^2$ for outcomes in $\mathbb{R}_2$. The left image represents $SS_{tot.}$: the red dots are data points ($\boldsymbol{y}_i$); the black dot is the mean ($\bar{\boldsymbol{y}}$); the line segments represent the Euclidean distance from each $\boldsymbol{y}_i$ to $\bar{\boldsymbol{y}}$. $SS_{tot.}$ is obtained by squaring the length of each line segment and then adding the squared segments together. The right image represents $SS_{resid.}$: the blue dots are the fitted values under the model ($\boldsymbol{f}_i$); the line segments represent the Euclidean distance from each $\boldsymbol{f}_i$ to its corresponding $\boldsymbol{y}_i$. $SS_{resid.}$ is obtained by squaring the length of each line segment and then adding the squared segments together. 

```{r geometric_graphic, fig.cap = "Visualizing the geometric interpretation of R-squared: corresponds to an R-squared of 0.87"}
set.seed("8675309")

# Generate sample data poings
mymat <- data.frame(y1=rnorm(n=5, mean=5, sd=3), y2=rnorm(n=5, mean=3, sd=5))

# Generate "predicted" values by adding jitter
mymat$y1hat <- jitter(mymat$y1, factor=10)
mymat$y2hat <- jitter(mymat$y2, factor=10)

# Calculate mean point
mymat$y1bar <- mean(mymat$y1)
mymat$y2bar <- mean(mymat$y2)

# Calculate sums of squares
ssres <- sapply(1:nrow(mymat), function(j){
    (mymat$y1[ j ] -mymat$y1hat[ j ])^2 + (mymat$y2[ j ] - mymat$y2hat[ j ])^2
})

sst <- sapply(1:nrow(mymat), function(j){
    (mymat$y1[ j ] -mymat$y1bar[ j ])^2 + (mymat$y2[ j ] - mymat$y2bar[ j ])^2
})

ssm <- sapply(1:nrow(mymat), function(j){
    (mymat$y1bar[ j ] -mymat$y1hat[ j ])^2 + (mymat$y2bar[ j ] - mymat$y2hat[ j ])^2
})

# plot those suckers
par(mfrow = c(1, 2))

plot(mymat[ , c("y1", "y2" ) ], yaxt="n", xaxt="n", pch=19, col=rgb(1,0,0,0.5), 
     ylim=c(min(c(mymat$y2, mymat$y2hat)),max(mymat$y2, mymat$y2hat)),
     xlim=c(min(c(mymat$y1, mymat$y1hat)),max(mymat$y1, mymat$y1hat)),
     main="Total Sum of Squares")
points(mymat[ , c("y1bar", "y2bar") ], pch=19)
for(j in 1:nrow(mymat)){
    lines(c(mymat$y1[ j ],mymat$y1bar[ j ]), c(mymat$y2[ j ],mymat$y2bar[ j ]))
}


plot(mymat[ , c("y1", "y2" ) ], yaxt="n", xaxt="n", pch=19, col=rgb(1,0,0,0.5), 
     ylim=c(min(c(mymat$y2, mymat$y2hat)),max(mymat$y2, mymat$y2hat)),
     xlim=c(min(c(mymat$y1, mymat$y1hat)),max(mymat$y1, mymat$y1hat)),
     main="Residual Sum of Squares")
points(mymat[ , c("y1hat", "y2hat") ], pch=17, col=rgb(0,0,1,0.5))
for(j in 1:nrow(mymat)){
    lines(c(mymat$y1[ j ],mymat$y1hat[ j ]), c(mymat$y2[ j ],mymat$y2hat[ j ]))
}

# 1 - sum(ssres)/sum(sst)
```

The geometric interpretation of $R^2$ is similar to the "explained-variance" interpretation. When $SS_{resid.} = 0$, then the model is a perfect fit for the data and $R^2 = 1$. If $SS_{resid.} = SS_{tot.}$, then $R^2 = 0$ and the model is no better than just guessing $\bar{y}$. When $0 < SS_{resid} < SS_{tot}$, then the model is a better fit for the data than a naive guess of $\bar{\boldsymbol{y}}$. In a non-linear or multi-dimensional model, it is possible for $SS_{resid.} > SS_{tot.}$. In this case, $R^2$ is negative, and guessing $\bar{\boldsymbol{y}}$ is better than using the model.

### Extending $R^2$ to Topic Models
An $R^2$ for topic models follows from the geometric interpretation of $R^2$. For a document, $d$, the observed value, $\boldsymbol{x}_d$, is a vector of integers counting the number of times each token appears in $n_d$ draws. The document's fitted value under the model follows that $\boldsymbol{x}_d$ is the outcome of a multinomial random variable. The fitted value is 

\begin{align}
  \boldsymbol{f}_d = 
    \mathbb{E}(\boldsymbol{x}_d) = 
    n_d \odot \boldsymbol\theta_d \cdot \boldsymbol{B} 
\end{align}

The center of the documents in the corpus, $\bar{\boldsymbol{x}}$, is obtained by averaging the token counts across all documents. From this we obtain $R^2$. 

\begin{align}
    \bar{\boldsymbol{x}} &= \frac{1}{D}\sum_{d=1}^{D}\boldsymbol{x}_d\\
    SS_{tot.} &= \sum_{d=1}^D{d(\boldsymbol{x}_d, \bar{\boldsymbol{x}})^2}\\
    SS_{resid.} &= \sum_{d=1}^D{d(\boldsymbol{x}_d, \boldsymbol{f}_d)^2}\\
    R^2 &\equiv 1 - \frac{SS_{resid.}}{SS_{tot.} }
\end{align} 

### Pseudo Coefficients of Variation

Several pseudo coefficients of variation have been developed for models where the traditional $R^2$ is inappropriate. Some of these, such as Cox and Snell's $R^2$ [@cox1989analysis] or McFadden's $R^2$ [@mcfadden1977application] may apply to topic models. As pointed out in _UCLA's Institute for Digital Research and Education_, [@bruin2006faq]

\begin{quote}
These are `pseudo' R-squareds because they look like R-squared in the sense that they are on a similar scale, ranging from 0 to 1 (though some pseudo R-squareds never achieve 0 or 1) with higher values indicating better model fit, but they cannot be interpreted as one would interpret an OLS R-squared and different pseudo R-squareds can arrive at very different values. 
\end{quote}

The empirical section of this paper calculates an uncorrected McFadden's $R^2$ for topic models to compare to the standard (non-pseudo) $R^2$. McFadden's $R^2$ is defined as

\begin{align}
    R^2_{Mc} & \equiv 1 - \frac{ ln( L_{full} ) }{ ln( L_{restricted} ) }
\end{align} 

where $L_{full}$ is the estimated likelihood of the data under the model and $L_{restricted}$ is the estimated likelihood of the data free of the model. In the context of OLS, the restricted model is a regression with only an intercept term. For other types of models (such as topic models), care should be taken in selecting what "free of the model" means. 

For topic models, "free of the model" may mean that the words were drawn from a simple multinomial distribution, whose parameter is proportional to the relative frequencies of words in the corpus overall. This is the specification used for the emperical analysis in this paper.

## Empirical Evaluation of Topic Model $R^2$

This paper performs three analyses to empirically evaluate $R^2$ for topic models. Two analyses use Monte Carlo-simulated corpora and one uses a corpus of grants awarded through the Department of Health and Human Services. This latter corpus was obtained from the National Institutes of Health _NIH ExPORTER_ database. [@nih] The first analysis uses simulated corpora to observe how the properties of training corpora influence $R^2$. The second analysis uses these same simulated corpora to compare $R^2$ as commonly-defined to McFadden's $R^2$ in the context of topic modeling. The final analysis compares the $R^2$ values of various models constructed on the NIH corpus.

### Monte Carlo-Simulated Corpora

It is possible to simulate corpora that share some key statistical properties of  human-generated language using the functional form of a topic model. To do so, one must set $\boldsymbol\beta$ such that its entries are proportional to a power law. The result is a corpus whose relative term frequencies follow Zipf's law of language [@zipf1949]. The derivation of this result is included in Appendix[X].

This method generates a corpus of $D$ documents, $V$ tokens, and $K$ topics through the following stochastic process:

1. Initialize 
  $\\ \boldsymbol\phi_k \sim \text{Dirichlet}(\boldsymbol\beta)\\$
  $\boldsymbol\theta_d \sim \text{Dirichlet}(\boldsymbol\alpha)$
2. Then for each document draw
  $\\ n_d \sim \text{Poisson}(\lambda)$
3. Finally, for each document draw the following $n_d$ times
  $\\ z_{d,n} \sim \text{Multinomial}(1, \boldsymbol\theta_d)\\$
  $w_{d,n} \sim \text{Multinomial}(1, \boldsymbol\phi_{z_{d,n}})$

The words for document $d$ are populated by sampling with replacement from $z_{d,n}$ and $w_{dk,n}$ for $n_d$ iterations. The parameters $V$, $K$, and $\lambda$ may be varied to adjust the corpus properties for the number of tokens, topics, and average document length respectively. Adjusting the shape and magnitude of $\boldsymbol\alpha$ and $\boldsymbol\beta$ affect the concentration of topics within documents and words within topics respectively.


### Comparing $R^2$ to McFadden's $R^2$

McFadden's pseudo $R^2$ is calculated for all simulated corpora for comparison to the standard $R^2$. McFadden's $R^2$ is a ratio of likelihoods. Various methods exist for calculating likelihoods of topic models [@buntine2009likelihood]. Most of these methods have a Bayesian perspective and incorporate prior information. Not all topic models are Bayesian, however. And in the case of simulated corpora, the exact data-generating parameters are known a priori. As a result, likelihoods calculated for this paper follow the simplest definition: they represent the probability of observing the generated data, given the (known or estimated) multinomial parameters of the model. The "model-free" likelihood assumes that each document is generated by drawing from a single multinomial distribution. The parameters of this "model-free" distribution are proportional to the frequency of each token in the data. 

### Empirical Properties of $R^2$ for Topic Models

The $R^2$ for topic models has the following empirical properties: It is bound between $-\infty$ and 1. $R^2$ is invariant to the \textit{true} number of topics in the corpus. $R^2$ increases with the \textit{estimated} number of topics using LDA; this indicates that there might be a risk of overfit from a model with too many topics (discussed in more detail in the next section). $R^2$ decreases as the vocabulary size of the corpus increases. $R^2$ increases as the average document length increases. (See Fig. 2, Fig. 3, Fig. 4, and Fig. 5.)

```{r k_plot, fig.cap = "Varying the number of topics on a simulated corpus"}

load("data-derived/r-squared/simulation_metrics.RData")

plot(k_m[, c("K", "r2")], type = "o", lwd = 3, col = "red",
     main = "",
     xlab = "Number of topics", ylab = "R-squared",
     ylim = c(0,1))

lines(k_m[, c("K", "r2_mac")], type = "o", lty = 2, lwd = 3, col = "blue")

legend("topright", legend = c("Standard", "McFadden's"),
       lwd = 3, lty = c(1,2), col = c("red", "blue"))

```


```{r l_plot, fig.cap = "Varying average document length on a simulated corpus"}
plot(lambda_m[, c("len", "r2")], type = "o", lwd = 3, col = "red",
     main = "",
     xlab = "Average document length", ylab = "R-squared",
     ylim = c(0,1))

lines(lambda_m[, c("len", "r2_mac")], type = "o", lty = 2, lwd = 3, col = "blue")

legend("topright", legend = c("Standard", "McFadden's"),
       lwd = 3, lty = c(1,2), col = c("red", "blue"))

```

```{r v_plot, fig.cap = "Varying vocabulary size on a simulated corpus"}
plot(v_m[, c("V", "r2")], type = "o", lwd = 3, col = "red",
     main = "",
     xlab = "Number of unique tokens", ylab = "R-squared",
     ylim = c(0,1))

lines(v_m[, c("V", "r2_mac")], type = "o", lty = 2, lwd = 3, col = "blue")

legend("topright", legend = c("Standard", "McFadden's"),
       lwd = 3, lty = c(1,2), col = c("red", "blue"))

```

```{r d_plot, fig.cap = "Varying number of documents on a simulated corpus"}
plot(d_m[, c("D", "r2")], type = "o", lwd = 3, col = "red",
     main = "",
     xlab = "Number of documents", ylab = "R-squared",
     ylim = c(0,1))

lines(d_m[, c("D", "r2_mac")], type = "o", lty = 2, lwd = 3, col = "blue")

legend("topright", legend = c("Standard", "McFadden's"),
       lwd = 3, lty = c(1,2), col = c("red", "blue"))

```

Most of these empirical properties were obtained by using simulated corpora, as described earlier in the paper, with one exception. The default parameter settings for Monte Carlo simulation are $K = 50$ topics, $D = 2{,}000$ documents, $V = 5{,}000$ tokens, and $\lambda = 500$ for the average document length, which is distributed $\text{Poisson}(\lambda)$. Each parameter was varied, holding other parameters constant. In each case, Monte Carlo simulation generates a document term matrix for the corpus while the parameters for generating the documents are known. $R^2$ is calculated for each simulated corpus, using the population parameters. These $R^2$ metrics represent a best-case scenario, avoiding misspecification and other pathologies present with topic model estimation algorithms. 

McFadden's pseudo $R^2$ is also calculated for these simulated data. McFadden's $R^2$ is generally lower than the standard $R^2$. McFadden's $R^2$ is subject to the common problem of many pseudo $R^2$ measures; its true upper bound is less than one. This makes sense. If McFadden's $R^2$ were to equal 1, then the likelihood of the data would be 1 which is impossible. It is never the case that a likelihood will equal 1. Given the scale of linguistic data, the likelihood will always be significantly less than 1. Therefore, a scale correction measure is needed, making McFadden's $R^2$ more complicated. 

McFadden's $R^2$ increases slightly with the number of true topics; this is problematic. When the scale of an $R^2$ varies with \textit{known} properties of the data, such as the number of documents, vocabulary size, average document length, etc. scale correction measures are possible. However, when the metric varies with an \textit{unknown} property, such as the number of latent topics, then a scale correction is not possible. For this reason alone, McFadden's $R^2$ is undesirable. Other properties of McFadden's $R^2$ are consistent with the properties of the standard $R^2$

These properties of $R^2$ suggest a change in focus for the topic modeling research community. Document length is an important factor in model fit whereas the number of documents is not. When choosing between fitting a topic model on short abstracts, full-page documents, or multi-page papers, it appears that longer documents are better. Second, since model fit is invariant for corpora over 1,000 documents (our lower bound for simulation), taking a sample from a large corpus may yield reasonable estimates of the larger corpus's parameters. The topic modeling community has heretofore focused on scalability to large corpora of hundreds-of-thousands to millions of documents. There has been less focus on document length. 

## Comparison of Simulated Data with the NIH Corpus

$R^2$ increases with the \textit{estimated} number of topics using LDA. This indicates a risk of model overfit with respect to the number of estimated topics. To evaluate the effect of $R^2$ on the number of estimated topics, LDA models were fit to two corpora. The first corpus is simulated, using the parameter defaults: $K = 50$, $D = 2{,}000$, $V = 5{,}000$, and $\lambda = 500$. The second corpus is on the abstracts of 1,000 randomly-sampled research grants for fiscal year 2014 in the National Institutes of Health's ExPORTER database. In both cases, LDA models are fit to the data estimating a range of $K$. For each model, $R^2$ and the log likelihood are calculated. The known parameters for this NIH corpus are $D = 1{,}000$, $V = 8,751$, and the median document length is 222 words. This vocabulary has standard stop words removed,[^snowballstopwords] is tokenized to consider only unigrams, and excludes tokens that appear in fewer than 2 documents.

[^snowballstopwords]: 175 English stop words from the snowball stemmer project. http://snowball.tartarus.org/algorithms/english/stop.txt

The same likelihood calculation is used here, as described above. This likelihood calculation excludes prior information typically used when calculating the likelihood of an LDA model. This is perhaps not the optimal method for calculating likelihoods when using a Bayesian model. However, this prior information is excluded here for two reasons. First, it is consistent with the method used for calculating McFadden's $R^2$ earlier in the paper. Second, this log likelihood can be calculated exactly. The likelihood of an LDA model including the prior is contained within an intractable integral. Various approximations have been developed [@buntine2009likelihood]. However, there is some risk that a comparison of an approximated likelihood to $R^2$ may be biased by the approximation method. 


```{r sim_figure, fig.cap="Comparison of R2 and log likelihood for LDA models fit on a simulated corpus"}


load("data-derived/r-squared/sim_out.RData")

par(mar = c(5,4,4,4) + 0.3)

plot(seq(from = 35, to=70, by=5),
     sim_out$r2, 
     type = "o", 
     lwd = 3,
     pch = 19,
     col = rgb(1,0,0,0.5),
     ylab = "R-squared",
     xlab = "Number of topics",
     main = "Simulated Corpus")

par(new = TRUE)

plot(seq(from = 35, to=70, by=5),
     sim_out$ll, 
     type = "o", 
     lwd = 3,
     col = rgb(0,0,1,0.5),
     lty = 2,
     pch = 17,
     ylab = "",
     xlab = "",
     yaxt = "n",
     xaxt = "n",
     main = "")

legend("bottomright", 
       legend=c("R-squared", "Log Likelihood"), 
       lty=c(1,2), 
       lwd=3, pch=c(19,17), col=c(rgb(1,0,0,0.5), rgb(0,0,1,0.5)))

axis(side=4, at = pretty(range(sim_out$ll)))
mtext("Log Likelihood", side=4, line=3)

```

```{r nih_figure, fig.cap="Comparison of R2 and log likelihood for LDA models fit on the NIH corpus"}

load("data-derived/r-squared/nih_out.RData")

par(mar = c(5,4,4,4) + 0.3)

plot(seq(from = 50, to=500, by=50),
     nih_out$r2, 
     type = "o", 
     lwd = 3,
     pch = 19,
     col = rgb(1,0,0,0.5),
     ylab = "R-squared",
     xlab = "Number of topics",
     main = "NIH Corpus")

par(new = TRUE)

plot(seq(from = 50, to=500, by=50),
     nih_out$ll, 
     type = "o", 
     lwd = 3,
     col = rgb(0,0,1,0.5),
     lty = 2,
     pch = 17,
     yaxt = "n",
     xaxt = "n",
     ylab = "",
     xlab = "",
     main = "")

legend("bottomright", 
       legend=c("R-squared", "Log Likelihood"), 
       lty=c(1,2), 
       lwd=3, pch=c(19,17), col=c(rgb(1,0,0,0.5), rgb(0,0,1,0.5)))

axis(side=4, at = pretty(range(nih_out$ll)))
mtext("Log Likelihood", side=4, line=3)

```

Fig. 6 and Fig. 7 depict $R^2$ and the log likelihood over a range of estimated $K$ for both corpora. Fig. 6 corresponds to the simulated corpus. Fig. 7 corresponds to the NIH corpus. From both images, we see that $R^2$ and the log likelihood both increase with the number of estimated topics. The scale of $R^2$ is comparable between the simulated corpus and the NIH corpus, in spite of their differences in size. In the case of the simulated corpus, we know that the true number of topics is 50. However, this is not clear from observing $R^2$ or the log likelihood. It does not appear that $R^2$ or this "raw" likelihood can help find the true number of topics.

@chang2009tea observe that there may be a trade-off between model fit and human interpretation. Specifically, they find that humans can more easily interpret models with fewer topics. This may be true. As discussed in an earlier section, non-$R^2$ goodness of fit measures are not readily comparable across corpora and models. Because $R^2$ is comparable, it is now possible to quantify how much goodness of fit is lost when K is lowered. This does not address any issues arising from a pathological misspecification of the model, an "incorrect" K, for example. In the case of the NIH corpus, $R^2$ goes from 0.27 to 0.2 when the number of estimated topics is lowered from 200 to 100.


## Conclusion

$R^2$ has many advantages over standard goodness of fit measures commonly used in topic modeling. Current goodness of fit measures are difficult to interpret, compare across corpora, and explain to lay audiences. $R^2$ does not have any of these issues. Its scale is effectively bounded between 0 and 1, as negative values (though possible) are rare and indicate extreme model misspecification. $R^2$ may be used to compare models of different corpora, if necessary. Scientifically-literate lay audiences are almost uniformly familiar with $R^2$ in the context of linear regression; the topic model $R^2$ has a similar interpretation, making it an intuitive measure. 

The standard (geometric) interpretation of $R^2$ is preferred to McFadden's pseudo $R^2$. The effective upper bound for McFadden's $R^2$ is considerably smaller than 1. A scale correction measure is needed. Also, it is debatable which likelihood calculation(s) are most appropriate. These issues make McFadden's $R^2$ complicated and subjective. However, a primary motivation for deriving a topic model $R^2$ is to remove the complications that currently hinder evaluating and communicating the fidelity with which topic models represent observed text. Most problematically, McFadden's $R^2$ varies with the number of \textit{true} topics in the data. It is therefore unreliable in practice where the true number of topics in unknown.

Lack of consistent evaluation metrics has limited the use of topic models as a mature statistical method. The development of an $R^2$ for topic modeling is no silver bullet. However it represents a step towards establishing consistency and rigor in topic modeling. This paper proposes reporting $R^2$ as a standard metric alongside topic models, as is typically done with OLS.

```{r clear-r2-data}
rm(list =)
```

