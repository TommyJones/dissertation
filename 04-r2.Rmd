# Coefficient of Determination for Topic Models {#r2}

According to an often-quoted but never cited definition, "the goodness of fit of a statistical model describes how well it fits a set of observations. Measures of goodness of fit typically summarize the discrepancy between observed values and the values expected under the model in question."[^goodness] Goodness of fit measures vary with the goals of those constructing the statistical model. Inferential goals may emphasize in-sample fit while predictive goals may emphasize out-of-sample fit. Prior information may be included in the goodness of fit measure for Bayesian models, or it may not. Goodness of fit measures may include methods to correct for model over fitting. In short, goodness of fit measures the performance of a statistical model against the ground truth of observed data. Fitting the data well is generally a necessary---though not sufficient---condition for trust in a statistical model, whatever its goals.

[^goodness]: This quote appears verbatim on Wikipedia and countless books, papers, and websites. I cannot find its original source.

Many researchers have eschewed goodness of fit measures as metrics of evaluating topic models. This is unfortunate. Goodness of fit is often the first line of defense against a pathologically misspecified model. If the model does not fit the data well, can it be relied on for inference or interpretation? Of course goodness of fit is not the only, perhaps not even the most important, measure of a good model. In fact a model that fits its training data too well is itself problematic. Nevertheless, a consistent and easily-interpreted measure of goodness of fit for topic models can serve to demystify the dark art of topic modeling to otherwise statistically literate audiences. 

Goodness of fit manifests itself in topic modeling through word frequencies. Topic models are not fully unsupervised methods. If they were unsupervised, this would mean that no observations exist upon which to compare a model’s fitted values. However, probabilistic topic models are ultimately generative models of word frequencies [@blei2002lda]. The expected value of word frequencies in a document under a topic model is given by the expected value of a multinomial random variable. The outcomes that can be compared to the predictions, then, are the word frequencies themselves. Most goodness of fit measures in topic modeling are restricted to in-sample fit. Yet some out-of-sample measures have been developed [@buntine2009likelihood].

For any probabilistic topic model, the following relationship holds

\begin{align}
  \mathbb{E}(\boldsymbol{X}) &= \boldsymbol{n} \odot \boldsymbol\Theta \cdot \boldsymbol{B}
\end{align}

where $\boldsymbol{X}$ is a matrix of observed word frequencies, $\boldsymbol{n}$ is a $D$-length vector whose $d$-th entry is the number of terms in the $d$-th document, $\boldsymbol\Theta$ is a matrix of estimated topic distributions over documents, and $\boldsymbol{B}$ is a matrix of estimated word distributions over topics. Also$\odot$ denotes elementwise multiplication whereas $\cdot$ is the typical dot product.

## Related Work

Evaluation methods for topic models can be broken down into three categories: manual inspection, intrinsic evaluation, and extrinsic evaluation [@shi2019eval]. Manual inspection involves human judgement upon examining the outputs of a topic model. Intrinsic evaluation measures performance based on model internals and its relation to training data. R-squared is an intrinsic evaluation method. Extrinsic methods compare model outputs to external information not explicitly modeled, such as the document class. 

Manual inspection is subjective but closely tracks how many topic models are used in real-world applications. Most research on topic model evaluation has focused on presenting ordered lists of words that meet human judgement about words that belong together. For each topic, words are ordered from the highest value of $\boldsymbol\beta_k$---the multinomial parameter for the $k$-th topic---to the lowest (i.e. the most to least frequent in each topic). In [@chang2009tea] the authors introduce the "intruder test" to evaluate the reasonableness of a topic's words appearing together.  Judges are shown a few high-probability words in a topic, with one low-probability word mixed in. Judges must find the low-probability word, the intruder. They then repeat the procedure with documents instead of words. A good topic model should allow judges to easily detect the intruders.

One class of intrinsic evaluation methods attempts to approximate human judgment. These metrics are called "coherence" metrics. Coherence metrics attempt to approximate the results of intruder tests in an automated fashion. Researchers have put forward several coherence measures. These typically compare pairs of highly-ranked words within topics. Röder et al. evaluate several of these [@roder2015coherence]. They have human evaluators rank topics by quality and then compare rankings based on various coherence measures to the ranking of the evaluators. They express skepticism that existing coherence measures are sufficient to assess topic quality. In a paper presented at the conference of the Association for Computational Linguistics (ACL), Lau, Newman, and Baldwin [@lau2014machine] find that normalized pointwise mutual information (NPMI) is a coherence metric that closely resembles human judgement. 

Other popular intrinsic methods are types of goodness of fit. The primary goodness of fit measures in topic modeling are likelihood metrics. Likelihoods, generally the log likelihood, are naturally obtained from probabilistic topic models. Researchers have used likelihoods to select the number of topics [@griffiths2004scientific], compare priors [@wallach2009rethinking], or otherwise evaluate the efficacy of different modeling procedures [@asuncion2012smoothing] [@nguyen2014sometimes]. A popular likelihood method for evaluating out-of-sample fit is called perplexity. Perplexity measures a transformation of the likelihood of the held-out words conditioned on the trained model.

The most common extrinsic evaluation method is to compare topic distributions to known document classes. The most prevalent topic in each document is taken as a document’s topical classification. Then, researchers will calculate precision, recall, and/or other diagnostic statistics for classification as measures of a topic model's success.

Though useful, prevalent evaluation metrics in topic modeling are difficult to interpret, are inappropriate for use in topic modeling, or cannot be produced easily. Intruder tests are time-consuming and costly, making intruder tests infeasible to conduct regularly. Coherence is not primarily a goodness of fit measure. AUC, precision, and recall metrics mis-represent topic models as binary classifiers. This misrepresentation ignores one fundamental motivation for using topic models: allowing contexts to contain multiple topics. This approach also requires substantial subjective judgement. Researchers must examine the high-probability words in a topic and decide whether it corresponds to the corpus topic tags or not.

Likelihoods have an intuitive definition: they represent the probability of observing the training data if the model is true. Yet properties of the underlying corpus influence the scale of the likelihood function. Adding more contexts, having a larger vocabulary, and even having longer contexts all reduce the likelihood. Likelihoods of multiple models on the same corpus can be compared. (Researchers often do this to help select the number of topics for a final model [@griffiths2004scientific].) Topic models on different corpora cannot be compared, however. One corpus may have 1,000 contexts and 5,000 words, while another may have 10,000 contexts and 25,000 words. The likelihood of a model on the latter corpus will be much smaller than a model on the former simply because the latter has more degrees of variability. Yet this does not indicate the model on the latter corpus is a worse fit; the likelihood function is simply on a different scale. Perplexity is a transformation of the likelihood often used for out-of-sample contexts. The transformation makes the interpretation of perplexity less intuitive than a raw likelihood. Further, perplexity’s scale is influenced by the same factors as the likelihood.

## The Coefficient of Determination: $R^2$

The coefficient of determination is a popular, intuitive, and easily-interpretable goodness of fit measure. The coefficient of determination, denoted $R^2$, is most common in ordinary least squares (OLS) regression. However, researchers have developed $R^2$ and several pseudo $R^2$ measures for many classes of statistical models. The largest value of $R^2$ is 1, indicating a model fits the data perfectly. The formal definition of $R^2$ (below) is interpreted---without loss of generality---as the proportion of _variability_ in the data that is explained by the model. For linear models with outcomes in $\mathbb{R}_1$, $R^2$ is bound between 0 and 1 and is the proportion of _variance_ in the data explained by the model [@neter1996applied]. Even outside of the context of a linear model, $R^2$ retains its maximum of 1 and its interpretation as the proportion of explained variability. Negative values of $R^2$ are possible for non-linear models or models in $\mathbb{R}_M$ where $M > 1$. These negative values indicate that simply guessing the mean outcome is a better fit than the model.

### The Standard Definition of $R^2$

For a model, $f$, of outcome variable, $y$, where there are $N$ observations, $R^2$ is derived from the following:

\begin{align}
  \bar{y} &= \frac{1}{N}\sum_{i=1}^{N}y_i\\
  SST &= \sum_{i=1}^N{(y_i-\bar{y})^2}\\
  SSR &= \sum_{i=1}^N{(f_i-y_i)^2}
\end{align}

The standard definition of $R^2$ is a ratio of summed squared errors. 

\begin{align}
    R^2 \equiv 1 - \frac{SSR}{SST}
\end{align} 

$SST$ and $SSR$ are known as the total sum of squares and residual sum of squares, respectively.

### A Geometric Interpretation of $R^2$

$R^2$ may be interpreted geometrically as well. $SST$ is the total squared-Euclidean distance from each $y_i$ to the mean outcome, $\bar{y}$. Then $SSR$ is the total squared-Euclidean distance from each $y_i$ to its predicted value under the model, $f_i$. Recall that for any two points $\boldsymbol{p}, \boldsymbol{q} \in \mathbb{R}_M$

\begin{align}
	d(\boldsymbol{p},\boldsymbol{q}) = \sqrt{\sum_{i=1}^M{(p_i - q_i)^2}}
\end{align}

where $d(\boldsymbol{p}, \boldsymbol{q})$ denotes the Euclidean distance between $\boldsymbol{p}$ and $\boldsymbol{q}$. $R^2$ is often taught in the context of OLS where $y_i, f_i \in \mathbb{R}_1$. In that case, $d(y_i, f_i) = \sqrt{(y_i - f_i)^2}$; by extension $d(y_i, \bar{y}) = \sqrt{(y_i - \bar{y})^2}$. In the multidimensional case where $\boldsymbol{y}_i, \boldsymbol{f}_i \in \mathbb{R}_M; M > 1$, then $\bar{\boldsymbol{y}} \in \mathbb{R}_M$ represents the point at the center of the data in $\mathbb{R}_M$.[^r2oneD]

[^r2oneD]: In the one-dimensional case, $y_i , f_i \in \mathbb{R}_1$, $SSR$ can be considered the squared-Euclidean distance between the $n$-dimensional vectors $y$ and $f$. However, this relationship does not hold when $\boldsymbol{y}_i , \boldsymbol{f}_i \in \mathbb{R}_M ; M > 1$.


We can rewrite $R^2$ using the relationships above. Note than now $\bar{\boldsymbol{y}}$ is now a vector with $M$ entries. The $j$-th entry of $\bar{\boldsymbol{y}}$ is averaged across all $N$ observations. i.e $\bar{y}_j = \frac{1}{N} \sum_{i=1}^{N} y_{i,v}$. From there we have:

\begin{align}
    \bar{\boldsymbol{y}} &= \frac{1}{N} \sum_{i=1}^{N} \boldsymbol{y}_i \\ 
    SST &= \sum_{i=1}^N{d(\boldsymbol{y}_i, \bar{\boldsymbol{y}}})^2\\
    SSR &= \sum_{i=1}^N{d(\boldsymbol{y}_i, \boldsymbol{f}_i)^2}\\
    \Rightarrow R^2 & \equiv 1 - \frac{SSR}{SST}
\end{align}

Figure \@ref(fig:geometric-graphic) visualizes the geometric interpretation of $R^2$ for outcomes in $\mathbb{R}_2$. The left image represents $SST$: the red dots are data points ($\boldsymbol{y}_i$); the black dot is the mean ($\bar{\boldsymbol{y}}$); the line segments represent the Euclidean distance from each $\boldsymbol{y}_i$ to $\bar{\boldsymbol{y}}$. $SST$ is obtained by squaring the length of each line segment and then adding the squared lengths together. The right image represents $SSR$: the blue dots are the fitted values under the model ($\boldsymbol{f}_i$); the line segments represent the Euclidean distance from each $\boldsymbol{f}_i$ to its corresponding $\boldsymbol{y}_i$. $SSR$ is obtained by squaring the length of each line segment and then adding the squared segments together. 

```{r geometric-graphic, fig.cap = "Visualizing the geometric interpretation of R-squared: corresponds to an R-squared of 0.87", fig.width = 6, fig.height = 4}
### Geometric R^2 example picture ----

set.seed("8675309")

# Generate sample data points
mymat <- tibble(
  y1 = rnorm(n = 5, mean = 5, sd = 3), 
  y2 = rnorm(n = 5, mean = 3, sd = 5)
) |>
  mutate(
    y1hat = jitter(y1, factor = 10),
    y2hat = jitter(y2, factor = 10),
    y1bar = mean(y1),
    y2bar = mean(y2)
  ) |>
  mutate(
    ssres = (y1 - y1hat) ^ 2 + (y2 - y2hat) ^ 2,
    sst = (y1 - y1bar) ^ 2 + (y2 - y2bar) ^ 2,
    ssm = (y1bar - y1hat) ^ 2 + (y2bar - y2hat) ^ 2
  )

# plot those suckers
par(mfrow = c(1, 2))

plot(mymat[ , c("y1", "y2" ) ], yaxt="n", xaxt="n", pch=19, col=rgb(1,0,0,0.5), 
     ylim=c(min(c(mymat$y2, mymat$y2hat)),max(mymat$y2, mymat$y2hat)),
     xlim=c(min(c(mymat$y1, mymat$y1hat)),max(mymat$y1, mymat$y1hat)),
     main="Total Sum of Squares")
points(mymat[ , c("y1bar", "y2bar") ], pch=19)
for(j in 1:nrow(mymat)){
  lines(c(mymat$y1[ j ],mymat$y1bar[ j ]), c(mymat$y2[ j ],mymat$y2bar[ j ]))
}


plot(mymat[ , c("y1", "y2" ) ], yaxt="n", xaxt="n", pch=19, col=rgb(1,0,0,0.5), 
     ylim=c(min(c(mymat$y2, mymat$y2hat)),max(mymat$y2, mymat$y2hat)),
     xlim=c(min(c(mymat$y1, mymat$y1hat)),max(mymat$y1, mymat$y1hat)),
     main="Residual Sum of Squares")
points(mymat[ , c("y1hat", "y2hat") ], pch=17, col=rgb(0,0,1,0.5))
for(j in 1:nrow(mymat)){
  lines(c(mymat$y1[ j ],mymat$y1hat[ j ]), c(mymat$y2[ j ],mymat$y2hat[ j ]))
}

# 1 - sum(ssres)/sum(sst)
```

The geometric interpretation of $R^2$ is similar to the "explained variance" interpretation. When $SSR = 0$, then the model is a perfect fit for the data and $R^2 = 1$. If $SSR = SST$, then $R^2 = 0$ and the model is no better than just guessing $\bar{y}$. When $0 < SS_{resid} < SS_{tot}$, then the model is a better fit for the data than a naive guess of $\bar{\boldsymbol{y}}$. In a non-linear or multi-dimensional model, it is possible for $SSR > SST$. In this case, $R^2$ is negative, and guessing $\bar{\boldsymbol{y}}$ is better than using the model.

### Extending $R^2$ to Topic Models
An $R^2$ for topic models follows from the geometric interpretation of $R^2$. For a document, $d$, the observed value, $\boldsymbol{x}_d$, is a vector of integers counting the number of times each token appears in $n_d$ draws. The document's fitted value under the model follows that $\boldsymbol{x}_d$ is the outcome of a multinomial random variable. The fitted value is 

\begin{align}
  \boldsymbol{f}_d = 
    \mathbb{E}(\boldsymbol{x}_d) = 
    n_d \odot \boldsymbol\theta_d \cdot \boldsymbol{B} 
\end{align}

The center of the contexts in the corpus, $\bar{\boldsymbol{x}}$, is obtained by averaging the token counts across all contexts. From this we obtain $R^2$. 

\begin{align}
    \bar{\boldsymbol{x}} &= \frac{1}{D}\sum_{d=1}^{D}\boldsymbol{x}_d\\
    SST &= \sum_{d=1}^D{d(\boldsymbol{x}_d, \bar{\boldsymbol{x}})^2}\\
    SSR &= \sum_{d=1}^D{d(\boldsymbol{x}_d, \boldsymbol{f}_d)^2}\\
    R^2 &\equiv 1 - \frac{SSR}{SST}
\end{align} 

### Pseudo Coefficients of Variation

Several pseudo coefficients of variation have been developed for models where the traditional $R^2$ is inappropriate. Some of these, such as Cox and Snell's $R^2$ [@cox1989analysis] or McFadden's $R^2$ [@mcfadden1977application] may apply to topic models. As pointed out in _UCLA's Institute for Digital Research and Education_, [@bruin2006faq]

\begin{quote}
These are `pseudo' R-squareds because they look like R-squared in the sense that they are on a similar scale, ranging from 0 to 1 (though some pseudo R-squareds never achieve 0 or 1) with higher values indicating better model fit, but they cannot be interpreted as one would interpret an OLS R-squared and different pseudo R-squareds can arrive at very different values. 
\end{quote}

The experiments in this chapter calculate an uncorrected McFadden's $R^2$ for topic models to compare to the standard (non-pseudo) $R^2$. McFadden's $R^2$ is defined as

\begin{align}
    R^2_{Mc} & \equiv 1 - \frac{ ln( L_{full} ) }{ ln( L_{restricted} ) }
\end{align} 

where $L_{full}$ is the estimated likelihood of the data under the model and $L_{restricted}$ is the estimated likelihood of the data free of the model. In the context of OLS, the restricted model is a regression with only an intercept term. For other types of models (such as topic models), care should be taken in selecting what "free of the model" means. 

For topic models, "free of the model" may mean that the words were drawn from a simple multinomial distribution, whose parameter is proportional to the relative frequencies of words in the corpus overall. This is the specification used for the empirical analysis in this paper.

## Experiments

To evaluate $R^2$ for topic models, I perform several simulation experiments and two experiments with a real-world data set. The simulation experiments are based on 4,096 synthetic data sets drawn from the LDA data generating process. (See the previous chapter.) The real-world data set is a sample of 1,000 abstracts from grants awarded by the US National Institutes of Health (NIH) in 2014 [@nih].

### Simulation Analysis
Effective use of $R^2$ for topic models requires an understanding of its properties under varying conditions. I generate 4,096 (or $4^6$) synthetic corpora using LDA as a data generating process and compare how both $R^2$ and McFadden's $R^2$ change as properties of the simulated data change.

#### Synthetic Data Generation and Evaluation Metrics

Each data set is drawn from a model with a unique combination of the following hyper parameters:

1. $\sum_{v=1} ^ V \eta_v \in \{50, 250, 500, 1000\}$
2. $\sum_{k = 1} ^ K \alpha_k \in \{0.5, 1, 3, 5\}$
3. $N_d \sim \text{Pois}(\lambda)$ where $\lambda \in \{50, 100, 200, 400\}$
4. $V \in \{1000, 5000, 10000, 20000\}$
5. $D \in \{500, 1000, 2000, 4000\}$
6. $K \in \{25, 50, 100, 200\}$

I choose $\boldsymbol\eta$ to be proportional to a power law, consistent with Appendix 3 and and Zipf's law [@zipf1949]. I chose $\boldsymbol\alpha$ to be symmetric. The magnitudes of the Dirichlet hyperparameters, $\boldsymbol\eta$ and $\boldsymbol\alpha$ vary while keeping their shapes fixed. The magnitude of the parameter of a Dirichlet distribution plays a strong role in tuning the covariance between draws from that distribution. A smaller magnitude means larger variability between draws. In other words, if $\left(\sum_{v=1}^V \eta_v \right)$ is smaller, topics are less linguistically similar. If $\left(\sum_{v=1}^V \eta_v \right)$ is larger, topics are more linguistically similar. Similarly, $\left(\sum_{k=1}^K \alpha_k \right)$ tunes the topical similarity of documents.

To remove the effects of estimating a pathologically-misspecified model, I use the data generating multinomial parameters $\boldsymbol\Theta$ and $\boldsymbol{B}$ to calculate $R^2$, and McFadden's $R^2$. Thus, these calculations represent the best possible model for a given simulated data set.

McFadden's $R^2$ is a ratio of likelihoods. Various methods exist for calculating likelihoods of topic models [@buntine2009likelihood]. Most of these methods have a Bayesian perspective and incorporate prior information. Not all topic models are Bayesian, however [@hofmann1999probabilistic]. And in the case of simulated corpora, the exact data-generating parameters are known a priori. As a result, likelihoods calculated for here follow the simplest definition: they represent the probability of observing the generated data, given the (known or estimated) multinomial parameters of the model, $\boldsymbol\Theta$ and $\boldsymbol{B}$. The "model-free" likelihood assumes that each document is generated by drawing from a single multinomial distribution. The parameters of this model-free distribution are proportional to the frequency of each token in the data. 

#### Results

Each plot in Figure \@ref(fig:r2-sim-plot) shows how the distribution of $R^2$ and McFadden's $R^2$ change as each of the number of estimated topics ($K$), average context length ($\lambda$), vocabulary size ($V$), and number of contexts ($D$) changes using boxplots. The standard $R^2$ does not change with properties of the data except for the average context length. This indicates that one should expect, for example, a higher $R^2$ if analyzing a corpus of full-length articles compared to a corpus of tweets. However, McFadden's $R^2$ is invariant only to the number of contexts. It is positively correlated to the number of topics, average context length, and vocabulary size. This indicates that McFadden's $R^2$ suffers from the same problems as the likelihood function. Many properties of the underlying data affect the resulting metric. 

```{r r2-sim-plot, fig.cap = "Comparing the standard geometric $R^2$ to McFadden's using population parameters and sampled data. Standard $R^2$ is positively correlated with the average document length but invariant to the number of topics, number of unique words, and number of documents. McFadden's is correlated with all but the number of contexts.", fig.width = 6, fig.height = 6}

library(tidyverse)
library(patchwork)

### load simulated data metrics ----
sim_metrics <- read_rds("data-derived/zipf-analysis/sim-metrics.rds")

### plot R2 by number of topics ----

p1 <- sim_metrics |> 
  mutate(Nk = factor(Nk)) |>
  ggplot(aes(x = Nk, y = r2)) +
  geom_boxplot(fill = "#33a02c") + 
  ylab("") + 
  xlab("Num. Topics") +
  ylim(c(0,1)) + 
  ggtitle("Standard R-Squared") + 
  coord_flip()

p2 <- sim_metrics |> 
  mutate(Nk = factor(Nk)) |>
  ggplot(aes(x = Nk, y = r2_mac)) +
  geom_boxplot(fill = "#33a02c") + 
  ylab("") + 
  xlab("") +
  ylim(c(0,1)) + 
  ggtitle("McFadden's R-Squared") +
  coord_flip()

### plot R2 by document length ----

p3 <- sim_metrics |> 
  mutate(doc_length = factor(doc_length)) |>
  ggplot(aes(x = doc_length, y = r2)) +
  geom_boxplot(fill = "#1f78b4") + 
  ylab("") + 
  xlab("Avg. Context Len.") +
  ylim(c(0,1)) +
  coord_flip()

p4 <- sim_metrics |> 
  mutate(doc_length = factor(doc_length)) |>
  ggplot(aes(x = doc_length, y = r2_mac)) +
  geom_boxplot(fill = "#1f78b4") + 
  ylab("") + 
  xlab("") +
  ylim(c(0,1)) +
  coord_flip()

### plot R2 by number of unique tokens ----

p5 <- sim_metrics |> 
  mutate(Nv = factor(Nv)) |>
  ggplot(aes(x = Nv, y = r2)) +
  geom_boxplot(fill = "#a6cee3") + 
  ylab("") + 
  xlab("Vocab. Size") +
  ylim(c(0,1)) +
  coord_flip()

p6 <- sim_metrics |> 
  mutate(Nv = factor(Nv)) |>
  ggplot(aes(x = Nv, y = r2_mac)) +
  geom_boxplot(fill = "#a6cee3") + 
  ylab("") + 
  xlab("") +
  ylim(c(0,1)) +
  coord_flip()

### plot R2 by number of documents ----

p7 <- sim_metrics |> 
  mutate(Nd = factor(Nd)) |>
  ggplot(aes(x = Nd, y = r2)) +
  geom_boxplot(fill = "#b2df8a") + 
  ylab("") + 
  xlab("Num. Contexts") +
  ylim(c(0,1)) +
  coord_flip()

p8 <- sim_metrics |> 
  mutate(Nd = factor(Nd)) |>
  ggplot(aes(x = Nd, y = r2_mac)) +
  geom_boxplot(fill = "#b2df8a") + 
  ylab("") + 
  xlab("") +
  ylim(c(0,1)) +
  coord_flip()



plot(p1 + p2 + p3 + p4 + p5 + p6 + p7 + p8 + 
       plot_layout(guides = "collect", ncol = 2))


```


These properties of $R^2$ suggest that context length may be important factor in model fit whereas the number of contexts is not. When choosing between fitting a topic model on short abstracts, full-page contexts, or multi-page papers, it appears that longer contexts are better. Second, since model fit is invariant for corpora over 500 contexts (our lower bound for simulation), taking a sample from a large corpus may yield estimates of parameters that accurately reflect the variability in word frequencies from a larger corpus. The topic modeling community has heretofore focused on scalability to large corpora of hundreds-of-thousands to millions of contexts. There has been less focus on context length. 

### Emperical Analysis

The analysis below demonstrates the use of $R^2$ for topic models on a real world data set. The analyses above demonstrated the properties of $R^2$ with perfectly-specified models. The real world is far messier with the number of topics and adequate prior specifications unknown. The analysis below compares $R^2$ and the log likelihood at picking the number of topics and calculates the partial $R^2$ of each topic in a 50-topic model.

#### Data Description

The data set consists of a random sample of 1,000 abstracts of research grants awarded by the National Institutes of Health (NIH) in 2014. The NIH invests roughly $32 billion annually in biomedical research [@nihgrants]. After applying common preprocessing steps (described below) the data set contains 8,903 unique words with a mean of 226.7 words and a median of 223 words per abstract (i.e., context). 

#### Modeling Choices
All text is converted to lower case, common stop words are removed, and punctuation is stripped. The vocabulary is pruned on a per-document level, rather than for the whole corpus. Words that appear only once in a document are removed from that document's vocabulary. 

I construct ten LDA models from $K = 50$ to $K = 500$ topics, increasing by 50 topics each time. I use symmetric priors $\alpha_k = 0.1$ and $\eta_v = 0.05$. This corresponds to $\sum_k \alpha_k \in \{5, 10, 15, ..., 45, 50\}$ and $\sum_v \eta_v = 445.15$. The Gibbs sampler is run for 200 iterations and posteriors are averaged over the last 50 iterations[^weirdburnin1].

[^weirdburnin1]: It is common to burn in half of the total number of iterations. I chose to burn in over the last quarter for computational reasons. Specifically, I wanted at least 150 burn in iterations but for time complexity reasons, limited the total number of iterations to 200.

#### Results

$R^2$ increases with the \textit{estimated} number of topics using LDA. This indicates a risk of model overfit with respect to the number of estimated topics. To evaluate the effect of $R^2$ on the number of estimated topics, LDA models were fit to two corpora. The first corpus is simulated, using the parameter defaults: $K = 50$, $D = 2{,}000$, $V = 5{,}000$, and $\lambda = 500$. The second corpus is on the abstracts of 1,000 randomly-sampled research grants for fiscal year 2014 in the National Institutes of Health's ExPORTER database. In both cases, LDA models are fit to the data estimating a range of $K$. For each model, $R^2$ and the log likelihood are calculated. The known parameters for this NIH corpus are $D = 1{,}000$, $V = 8,751$, and the median document length is 222 words. This vocabulary has standard stop words removed,[^snowballstopwords] is tokenized to consider only unigrams, and excludes tokens that appear in fewer than 2 documents.

[^snowballstopwords]: 175 English stop words from the snowball stemmer project. http://snowball.tartarus.org/algorithms/english/stop.txt

The same likelihood calculation is used here, as described above. This likelihood calculation excludes prior information typically used when calculating the likelihood of an LDA model. (See, for example, the likelihood used in [@chen2015warplda].) However, this prior information is excluded here for two reasons. First, it is consistent with the method used for calculating McFadden's $R^2$ earlier in this chapter. Second, this log likelihood can be calculated exactly. The likelihood of an LDA model including the prior is contained within an intractable integral. Various approximations have been developed [@buntine2009likelihood]. However, there is some risk that a comparison of an approximated likelihood to $R^2$ may be biased by the approximation method. 


```{r sim-likelihood-figure, fig.cap="Comparison of R2 and log likelihood for LDA models fit on simulated corpora. Neither $R^2$ nor log likelihood are effective statistics for detecting the correct number of topics to estimate. The true number of topics is 200 in these corpora.", fig.width = 6, fig.height = 4}

# ### Simulated corpus likelihood plot ----
# sim_model_metrics <- read_rds("data-derived/r-squared/sim-model-output.rds")
# 
# p1 <- sim_model_metrics |>
#   mutate(Nk_est = factor(Nk_est)) |>
#   ggplot(aes(x = Nk_est, y = r2, fill = Nk_est)) +
#   geom_boxplot() +
#   theme(legend.position = "none")+
#   ylab("R-squared") +
#   xlab("") +
#   ggtitle("Simulated Corpora")
# 
# p2 <- sim_model_metrics |>
#   mutate(Nk_est = factor(Nk_est)) |>
#   ggplot(aes(x = Nk_est, y = ll, fill = Nk_est)) +
#   geom_boxplot() +
#   theme(legend.position = "none")+
#   ylab("Log Likelihood") +
#   xlab("Number of Estimated Topics")
# 
# plot(p1 / p2)


```

```{r nih-figure, fig.cap="Comparison of R-squared and McFadden's R-squared for LDA models fit on the NIH corpus over a range of topics. R-squared and McFadden's R-squared increase with the number of estimated topics, peaking for 350 topics and 300 topics, respectively. In both cases, the metric tapers slightly after its peak. McFadden's is higher than the standard R-squared for all values.", fig.width = 6, fig.height = 4}

### NIH Likelihood plot ----

nih_metrics <- read_rds("data-derived/r-squared/nih-metrics.rds")

p1 <- nih_metrics |>
  ggplot(aes(x = Nk, y = r2)) +
  geom_point(color = "#1f78b4") +
  geom_line(color = "#1f78b4") + 
  ggtitle("NIH Corpus") +
  ylab("R-squared") + 
  xlab("")

p2 <- nih_metrics |>
  ggplot(aes(x = Nk, y = r2_mac)) +
  geom_point(color = "#33a02c") +
  geom_line(color = "#33a02c") + 
  xlab("Number of Estimated Topics") + 
  ylab("McFadden's")

plot(p1 / p2)

```

One can calculate a partial $R^2$ for topic models, similar to a partial coefficient of determination for standard linear models. The partial coefficient of determination is calculated as

\begin{align}
  R^2_{partial} &=
    1 - \frac{SSR_{(full)}}{SSR_{(reduced)}}
\end{align}

where $SSR_{(full)}$ is the standard $SSR$ above and $SSR_{(reduced)}$ is the residual sum of squares with topic $k$ removed. 

A key difference between a partial $R^2$ for topic models and the one for standard linear models is how one obtains $SSR_{(reduced)}$. For a standard linear model, the variables are explicit. A variable is removed, a new model fitted, and $SSR_{(reduced)}$ calculated. However topics are latent variables and topic models like LDA are initialized at random during fitting. So instead of fitting a new model, the $k$-th column is removed from $\boldsymbol\Theta$ and $k$-th row removed from $\boldsymbol{B}$. The rows of $\boldsymbol\Theta$ are normalized to sum to 1. Then $SSR_{(reduced)}$ is calculated from the reduced $\boldsymbol\Theta$ and $\boldsymbol{B}$.

The left of Figure \@ref(fig:nih-partial-r2) plots the partial $R^2$ for the 10 topics contributing the most to the goodness of fit for a 50-topic model on the sample of NIH grant abstracts. The right of Figure \@ref(fig:nih-partial-r2) plots the correlation between probabilistic coherence and the partial $R^2$ of topics in the same 50-topic model. There is a slight positive correlation between a topic's partial $R^2$ and its probabilistic coherence in this model. I leave a deeper exploration between the two variables' relationship to future research.

```{r nih-partial-r2, fig.cap="10 topics with the highest partial $R^2$ from a 50-topic model of the NIH 2014 corpus.", fig.width = 6, fig.height = 4}
library(tidyverse)
library(patchwork)


partial_r2 <- read_rds("data-derived/r-squared/nih-partial-r2.rds")

nih_models <- read_rds("data-derived/r-squared/nih-models.rds")
  
partial_r2 <- 
  partial_r2 |>
  right_join(
    nih_models[sapply(nih_models, function(x) nrow(x$beta) == nrow(partial_r2))][[1]]$summary
  )

rm(nih_models)

p1 <- partial_r2 |> 
  mutate(topic_o = ordered(top_terms)) |> 
  mutate(topic_o = reorder(topic_o, partial_r2)) |> 
  top_n(10) |>
  ggplot(aes(x = topic_o, y = partial_r2)) + 
  geom_bar(stat = "identity", fill = "#33a02c") + 
  ylab("") +
  xlab("") +
  theme(axis.text.x = element_text(angle=45, hjust=1)) +
  coord_flip() +
  ggtitle("Partial R-Sq.")

p2 <- partial_r2 |>
  ggplot(aes(x = partial_r2, y = coherence)) +
  geom_point(color = "#1f78b4") +
  ylab("Coherence") +
  xlab("Partial R-Sq.") + 
  theme(axis.text.x = element_text(angle=45, hjust=1)) +
  ggtitle(
    paste("Cor:", round(cor(partial_r2$partial_r2, partial_r2$coherence), 2))
  )

plot(p1 + p2)
```


## Discussion

$R^2$ has many advantages over standard goodness of fit measures commonly used in topic modeling. Current goodness of fit measures are difficult to interpret, to compare across corpora, and to explain to lay audiences. $R^2$ does not have any of these issues. Its scale is effectively bounded between 0 and 1, as negative values (though possible) indicate extreme model misspecification. $R^2$ may be used to compare models of different corpora, if necessary. Scientifically-literate lay audiences are almost uniformly familiar with $R^2$ in the context of linear regression; the topic model $R^2$ has a similar interpretation, making it an intuitive measure. 

The standard (geometric) interpretation of $R^2$ is preferred to McFadden's pseudo $R^2$. The effective upper bound for McFadden's $R^2$ is considerably smaller than 1. A scale correction measure is needed. Also, it is debatable which likelihood calculation(s) are most appropriate. These issues make McFadden's $R^2$ complicated and subjective. However, a primary motivation for deriving a topic model $R^2$ is to remove the complications that currently hinder evaluating and communicating the fidelity with which topic models represent observed text. Most problematically, McFadden's $R^2$ varies with the number of \textit{true} topics in the data. It is therefore unreliable in practice where the true number of topics in unknown.

Lack of consistent evaluation metrics has limited the use of topic models as a mature statistical method. The development of an $R^2$ for topic modeling is no silver bullet. However, it represents a step towards establishing consistency and rigor in topic modeling. Going forward, I propose reporting $R^2$ as a standard metric alongside topic models, as is typically done with ordinary least squares regression.

```{r clear-r2-data}
rm(list = ls())
```

