# _tidylda_, an R Package {#tidylda}

Off the shelf implementations of LDA are plentiful, yet existing LDA implementations do not integrate well with the research in this dissertation. Moreover, putting the methods proposed in this dissertation in a package and hosting it on a well used package repository like CRAN [@cran] makes those methods accessible to a wider audience. And creating a topic modeling implementation supporting philosophies of the _tidyverse_ [@tidyverse] movement in the R ecosystem, which focus on making data analysis and computing tools work for humans, increases the accessiblity of these topic modeling methods. Having topic modeling software compatible with the _tidyverse_ ecosystem is in line with Boyd-Graber et al.'s cry for "automatic text analysis for the people" [@boydgraber2017applications, ch. 10.3].

_tidylda_ is a package for topic modeling that is compatible with the _tidyverse_ and includes methods contained in the previous chapters of this dissertation. _tidylda_'s Gibbs sampler is written in C++ for performance and offers several novel features. It also has methods for sampling from the posterior of a trained model, for more traditional Bayesian analyses..

## Related Work

### The "tidyverse" and "tidy" text mining
_tidylda_ takes its syntactic cues from an ecosystem of R packages known as _the tidyverse_. The tidyverse's goal is to "facilitate a conversation between a human and computer about data" [@tidyverse]. Packages in---and adjacent to---the tidyverse share a common design philosophy and syntax based on "tidy data" principles [@wickham2014tidy]. Tidy data has each variable in a column, each observation in a row, and each observational unit in a table. Extensions include the _broom_ package [@broom] for "tidying" up outputs from statistical models and the in-development _tidymodels_ ecosystem [@tidymodels] which extends the tidyverse philosophy to statistical modeling and machine learning workflows. 

Recently, Silge et al. articulated a "tidy data" framework for text analyses---the _tidytext_ package [@tidytextjoss]. Their approach has "one row per document per token". The _tidytext_ package provides functionality to tokenize a corpus, transform it into this "tidy" format, and manipulate it in various ways, including preparing data for input into some of R's many topic modeling packages. The _tidytext_ package also provides tidying functions in the style of _broom_ to harmonize outputs from some of R's topic modeling packages. _tidylda_ manages inputs and outputs in the flavor of _tidytext_ but in one self contained package.

### Topic modeling software in R
R has many packages for topic modeling. As far as I am aware, none are natively "tidy" though some have wrapper functions available in _tidytext_ for interoperability. In almost all cases---at least for R packages---these models support only symmetric $\boldsymbol\eta$ priors, though some support asymmetric $\boldsymbol\alpha$. 

The _textmineR_ package [@textminer] supports fitting models for LDA, correlated topic models [@blei2007ctm], and LSA. It also contains a suite of analysis functions that are (mostly) interoperable with other topic modeling packages in the R ecosystem. In many ways, _textmeineR_ is the predecessor of _tidylda_, supporting asymmetric priors for both $\boldsymbol\alpha$ and $\boldsymbol\eta$. But _textmineR_ does not support transfer learning nor is it fully consistent with the _tidyverse_ principles.

The _topicmodels_ package [@topicmodelspackage] supports fitting models for LDA and correlated topic models [@blei2007ctm] with both a collapsed Gibbs sampler and VEM. When using VEM, $\boldsymbol\alpha$ may be treated as a free parameter and estimated during fitting. It is designed to be interoperable with the _tm_ package [@tmjss], the oldest framework for text analysis in R.[^killitwithfire] _tidytext_ provides "tidier" functions to make the _topicmodels_ package interoperable with other frameworks, such as _quanteda_ [@quanteda], _text2vec_ [@text2vec], and more. 

[^killitwithfire]: From private conversations, there is an immense frustration with working with _tm_ within the R user community.

The _lda_ package [@chang2015lda] provides a collapsed Gibbs sampler for LDA, supervised LDA [@nguyen2015supervisedtm], and other less well-known models. It allows users to specify only symmetric $\boldsymbol\alpha$ and $\boldsymbol\eta$. Its syntax is esoteric and it requires text documents as input, but does not offer much flexibility in the way of pre-processing. It is generally not interoperable with other packages without significant programming on the part of its users. However, its sequential Gibbs sampler is one of the fastest. 

The _text2vec_ package [@text2vec] is a framework for very fast text pre-processing and modeling. _textmineR_ wraps _text2vec_ for its pre-processing functions. _text2vec_ implements LDA using the WarpLDA algorithm [@chen2015warplda], but it only allows symmetric priors. _text2vec_ also offers other models related to distributional semantics. Its syntax is also esoteric using R's _R6_ objects that reach back to actively running C++ code for performance reasons. One of _text2vec_'s novel features is that it implements many different coherence calculations; most packages implement one or none.

The _STM_ package [@roberts2019stm] implements VEM algorithms for structural topic models [@roberts2013stm] and correlated topic models [@blei2007ctm]. _STM_ is well-supported with interfaces in _tidytext_. It offers unique capabilities for model initialization somewhat analogous to transfer learning. Models may be initialized at random or from an LDA model that has run for a few iterations. _STM_ does not offer this as a fully-fledged "transfer learning" paradigm. Instead it is a flag the user sets at run time. _STM_ then produces the LDA model to hand off to the STM model internally. STM has several unique methods for setting priors but inspecting the documentation makes me believe that they are still symmetric, where applicable.

### Topic modeling software in other languages
There are many (many) programs to implement topic models in many languages. Anecdotally, the two most common are also two of the oldest: _MALLET_ and _Gensim_.

_MALLET_ [@mallet] stands for "MAchine Learning for LanguagE Toolkit." It is a Java program that implements LDA and many other models for working with text. Its LDA capabilities have a wrapper available in R---the _mallet_ package---which launches a Java Virtual Machine in the background while users interact with it from R. MALLET allows users to set symmetric priors, but has an option to estimate an asymmetric $\boldsymbol\alpha$ based on [@minka2000estimating]. Input must be text files, as it does all of its own pre-processing. MALLET's LDA implementation offers both a collapsed Gibbs sampler and distributed approximate Gibbs in the flavor of Newman et al. [@newman2009distributed]. _tidytext_ has _broom_-like functions to format the outputs of the _mallet_ package.

_Gensim_ [@gensim] is a Python package that implements many models for working with text data, including LDA, LSI, and HDP [@teh2006hdp]. Its LDA implementation is based on VEM and can be distributed across many compute nodes. Users can set flags for both $\boldsymbol\alpha$ and $\boldsymbol\eta$ to be estimated during fitting. $\boldsymbol\eta$ can be a scalar, vector, or matrix. To my knowledge, Gensim is the only other program that offers this option---other than _tidylda_. Gensim uses other Python packages for pre-processing. 

## _tidylda_'s Novel Features

### Model Initialization and Gibbs Sampling

_tidylda_'s Gibbs sampler has several unique features, described below.

**Non-uniform initialization:** Most LDA Gibbs samplers initialize by assigning words to topics and topics to documents (i.e., construct the $\boldsymbol{Cd}$ and $\boldsymbol{Cv}$ matrices) by sampling from a uniform distribution. This ensures initialization without incorporating any prior information. _tidylda_ incorporates the priors in its initialization. It begins by drawing $P(\text{topic}|\text{document})$ and $P(\text{word}|\text{topic})$ from Dirichlet distributions with parameters $\boldsymbol\alpha$ and $\boldsymbol\eta$, respectively. Then _tidylda_ uses the above probabilities to construct $P(\text{topic}|\text{word}, \text{document})$ and makes a single run of the Gibbs sampler to initialize $\boldsymbol{Cd}$ and $\boldsymbol{Cv}$. This non-uniform initialization powers tLDA, described in Chapter 5, by starting a Gibbs run near where the previous run left off. For initial models, it uses the user's prior information to tune where sampling starts.

**Flexible priors:** _tidylda_ has multiple options for setting LDA priors. Users may set scalar values for $\boldsymbol\alpha$ and $\boldsymbol\eta$ to construct symmetric priors. Users may also choose to construct vector priors for both $\boldsymbol\alpha$ and $\boldsymbol\eta$ for a full specification of LDA. Additionally, _tidylda_ allows users to set a matrix prior for $\boldsymbol\eta$, enabled by its implementation of tLDA. This enables users to set priors over word-topic relationships informed by expert input. The best practices for encoding expert input in this manner are not yet well studied. Nevertheless, this capability makes _tidylda_ unique among LDA implementations. 

**Burn in iterations and posterior averaging:** Most LDA Gibbs samplers construct posterior estimates of $\boldsymbol\Theta$ and $\boldsymbol{B}$ from $\boldsymbol{Cd}$ and $\boldsymbol{Cv}$'s values of the final iteration of sampling, effectively using a single sample. This is inconsistent with best practices from Bayesian statistics, which is to average over many samples from a stable posterior. _tidylda_ enables averaging across multiple samples of the posterior with the `burnin` argument. When `burnin` is set to a positive integer, _tidylda_ averages the posterior across all iterations larger than `burnin`. For example, if `iterations` is 200 and `burnin` is 150, _tidylda_ will return a posterior estimate that is an average of the last 50 sampling iterations. This ensures that posterior estimates are more likely to be representative than any single sample.

**Transfer learning with tLDA:** Finally, and as discussed previously, _tidylda_'s Gibbs sampler enables transfer learning with tLDA. The full specification of tLDA and details on its implementation in _tidylda_ are in Chapter 5.

### Tidy Methods

_tidylda_'s construction follows _Conventions of R Modeling Packages_ [@tidymodelsbook]. In particular, it contains methods for `print`, `summary`, `glance`, `tidy`, and `augment`, consistent with other "tidy" packages. These methods are briefly described below and are demonstrated in Appendix 6.

* `print`, `summary`, and `glance` return various summaries of the contents of a `tidylda` object, into which an LDA model trained with _tidylda_ is stored.
* `tidy` returns the contents of $\boldsymbol\Theta$, $\boldsymbol{B}$, or $\boldsymbol\Lambda$ (stored as `theta`, `beta`, and `lambda` respectively), as specified by the user, formatted as a tidy `tibble`, instead of a numeric matrix.
* `augment` appends model outputs to observational-level data. Taking the cue from _tidytext_ [@tidytextjss], "observational-level" data is one row per word per document. Therefore, the key statistic used by `augment` is $P(\text{topic}|\text{word}, \text{document})$. _tidylda_ calculates this as $\boldsymbol\Lambda \times P(\text{word}|\text{document})$, where $P(\text{word}|\text{document}_d) = \frac{\boldsymbol{x}_d}{\sum_{v=1}^V x_{d,v}}$. 

### Other Noteable Features

_tidylda_ has three evaluation metrics for topic models, two goodness-of-fit measures---$R^2$ as described in Chapter 4 and the log likelihood of the model given the data---and one coherence measure---probabilistic coherence. R-squared is calculated via the _mvrsquared_ package [@mvrsquared] and is a flag set during model fitting with `calc_r2 = TRUE`[^calcr2]. Similarly, the log likelihood of the model, given the data, is calculated at each Gibbs iteration if the user selects `calc_likelihood = TRUE` during model fitting. 

The coherence measure is called probabilistic coherence. (See Appendix 5.) Probabilistic coherence is bound between -1 and 1. Values near zero indicate that the top words in a topic are statistically independent from each other. Positive values indicate that the top words in a topic are positively correlated in a statistically-dependent manner. Negative values indicate that the words in a topic are negatively correlated with each other in a statistically-dependent manner. (In practice, negative values tend to be very near zero.)

[^calcr2]: Users can calculate $R^2$ after a model is fit by using the _mvrsquared_ package or calling `tidylda:::calc_lda_rsquared`. `calc_lda_rsquared` is an internal function to _tidylda_, requiring the package name followed by three colons, as is R's standard.

_tidylda_ enables traditional Bayesian uncertainty quantification by sampling from the posterior. The posterior distribution for $\boldsymbol\theta_d$ is $\text{Dirichlet}(\boldsymbol{Cd}_d + \boldsymbol\alpha)$ and the posterior distribution for $\boldsymbol\beta_k$ is $\text{Dirichlet}(\boldsymbol{Cv}_k + \boldsymbol\eta)$ (or $\text{Dirichlet}(\boldsymbol{Cv}_k + \boldsymbol\eta_k)$ for tLDA). _tidylda_ enables a `posterior` method for `tidylda` objects, allowing users to sample from the posterior to quantify uncertainty for estimates of estimated parameters.

_tidylda_ uses one of two calculations for predicting topic distributions (i.e., $\hat{\boldsymbol\theta}_d$) for new documents. The first, and default, is to run the Gibbs sampler, constructing a new $\boldsymbol{Cd}$ for the new documents but without updating topic-word distributions in $\boldsymbol{B}$. The second uses a dot product as described in Appendix 1. _tidylda_ acually uses the dot product prediction combined with the _non-uniform initialization_---described above---to initialize $\boldsymbol{Cd}$ when predicting using the Gibbs sampler.

## Discussion

While many other topic modeling packages exist, _tidylda_ is very user friendly and brings novel features. Its user friendliness comes from compatibility with the tidyverse. And _tidylda_ includes tLDA and other methods contained in the previous chapters of this dissertation. It also has methods for sampling from the posterior of a trained model, for more traditional Bayesian analyses. _tidylda_'s Gibbs sampler is written in C++ for performance.


