# Introduction and Motivation {#intro}

Human language is one of the most information rich sources of data that exists. Language is literally the medium humans use to communicate information to each other. And in an increasingly digitally connected world, the amount of text available for analysis has exploded. Improvements in computing power and algorithmic advances have driven staggering progress in machine learning tasks for natural language, including machine translation, question answering, automatic summarization, information extraction and more. 

Such advances have lead an increase in textual analysis in two relatively new, interdisciplinary fields---the "_digital humanities_" and "_computational social science_". The digital humanities represents a quantification of traditionally qualitative fields such as history, literature, communications, and the arts. Computational social science emphasizes computationally-intensive methods for disciplines including economics, political science, and psychology. Examples of textual analyses from these two new fields include statistical modeling of language in the _Pennsylvania Gazette_ from 1728 to 1800 [@newman2006probabilistic], tracking the evolution of the Greek word _kosmos_ from 700 BC onward [@perrone2019gasc], using language from the Federal Reserve Board's public press releases to predict the Fed's non-public economic forecasts [@ericsson2017predicting] and more. Applications to economics and social science were presented in recent Association for Computational Linguistics workshops [@emnlp-2019-economics] [@ws-2019-natural-language].

## Natural Language Statistics
The tools used for text analyses in these new fields derive from linguistics, computing, and statistics. Linguistics has the sub-field of _corpus linguistics_, the study of language as it appears in samples of "real world" text (corpora). Computing has the sub-field of _natural language processing_ (NLP) which concerns itself with the interactions of people and computers. A particularly relevant sub-field of NLP is _distributional semantics_, which quantifies semantic similarities in language in terms of relative frequencies of linguistic items (such as words). Yet in statistics no named sub-field exists. 

Statistics as a field is waking up to its role regarding linguistic data, however. The American Statistical Association (ASA) recently formed an interest group for text analysis in 2019 and elevated it a full section in 2022[^interestgroup] [@taig2019]. A year later, the Joint Statistics Meetings---the annual conference of the ASA and 11 other statistical associations---had over 20 sessions featuring text analysis research [@jsm2020]. This isn't to say the interest group caused the volume of text analysis research. Note that many of the references in this proposal publish in statistics journals---such research is happening anyway. But only recently has there been effort to organize a community of practice for text analyses under a statistics umbrella.

[^interestgroup]: I have been involved since the beginning and am the group's web master.

Rigorous statistical study of linguistic phenomena can elevate the digital humanities, linguistic applications to computational social science, and statistics itself. As yet, statistical applications to language, whether in linguistics, computation, or otherwise, are largely ad-hoc. Save a handful of empirical laws, there is little statistical theory guiding the modeling of textual data. What theory does exist generally does not inform specification or use of statistical or machine learning models of text. Instead, the field has relied on increasingly complex models, requiring tremendous computational power, to drive these advances.

To envision the art of the possible, consider the state of statistical theory in linear regression. Linear regression---and its related statistical theory---dates back to the early 1800's from the works of Legendre [@legendre1805nouvelles], Gauss [@angrist2008mostly], and Galton [@stanton2001galton]. We have formal statements of the assumptions required for valid statistical inference using linear regression. We have multitudes of diagnostic statistics to assess the degree these assumptions are violated in the data or with model specifications. There are a plethora of remediations to apply when key assumptions are violated so that valid statistical inference may still be made. And there are an abundance of statistical inferential methods built on top regression models used to detect structural breaks [@chow1960tests], calculate individual variable's contribution to the coefficient of determination [@anderson1994model], and on and on. We have been studying linear regression for a long time. As a result, we have an extremely powerful kit of statistical tools that are so easy to use that in 2020, we mostly take them for granted. Imagine what we could do if we brought this level of rigor to models and applications using language data in all of its abundance.

Again, statistics does not have a named sub-field dedicated to such study. Since naming a concept can give it power, I humbly submit "_Natural Language Statistics_", so named because it is distinct from task-based Natural Language Processing. Natural Language Statistics may focus on "traditional" statistical concerns such as constructing appropriate random samples, quantifying uncertainty about claims learned from data, developing rigorous evaluation metrics for models, and so on where language is the topic of study. Natural Language Statistics can build off of work already begun in linguistics, machine learning, and complexity theory which concerns itself with the study of complex phenomena and power laws.

Language is saturated with the statistics of power laws. Two empirical laws of language, Zipf's law and Heaps's law---covered in more detail later in this proposal---are two manifestations of power laws in language data. Power laws are extreme valued distributions, with significant mass in their tails. Power laws have extremely large variance. In fact, for many parameterizations, the second moment does not exist, meaning the variance tends towards infinity [@taleb2020, ch. 3]. Because of power laws in language, the law of large numbers operates very slowly. Very large samples are required to reliably estimate population parameters [@taleb2020, ch. 8]. As a consequence, most corpora (samples of language) are missing key information. 

Power laws in language might imply, then, that one needs external information for a thorough analysis of any one corpus. In fact, this is actually how humans learn from language. Consider the following thought experiment where one wants to learn about chemistry from an English-language textbook. Presumably, this person has good grasp on the English language already, having a vocabulary and covering subject matter greater than the book itself contains. This person then reads the textbook, learns about chemistry, and in the process updates and expands their knowledge of the English language.[^soundsbayesian] Thus, in addition to traditional statistical concerns like representative samples and reasonable model specification, Natural Language Statistics needs foundation models of language to be updated for analyzing finite corpora.

[^soundsbayesian]: This sounds philosophically Bayesian to me. Yet I see no reason why one needs to limit their study to the use of Bayesian statistical models.

## Fine Tuning Transfer Learning
In machine learning, starting with a base model and then updating its parameters with a new sample is called the _fine tuning paradigm of transfer learning_. Transfer learning is when a model is developed for one task---on one data set---and then re-used for another task and data set [@pan2009survey]. In the fine tuning paradigm, the base model is modified for the new task, allowed to update based on the new data, or both modified to the task and updated with new data [@bert2018]. 

Neural networks lend themselves naturally to fine tuning transfer learning. They are trained (or fit) using the iterative back propagation algorithm. Instead of initializing the model parameters---often called "weights"---at random, they are initialized at the same values they had in the base model. Then back propagation is resumed using the new data and all or some of the weights are allowed to update. This lets the analyst leverage information from a very large data set encoded in base model and adjust the parameters to fold in information in their new data set. Fine tuning transfer learning has been used for many years in deep learning models for computer vision, but it has recently gained widespread adoption in deep learning models for language.

Current state of the art natural language processing models belong to a class of deep neural networks called "transformers".[^technicallydistinct] Famous examples of transformers include BERT [@bert2018], XLM-R [@xlm-r], GPT-2 [@gpt2], GPT-3 [@gpt3], and more. In terms of raw accuracy for benchmark task-specific objectives, transformer models outperform other models in Natural Language Processing [@wolf2020transformer].

[^technicallydistinct]: Transformers get their name from the "transformer" sub-architecture for deep neural networks [@attention2017]. Technically, this is distinct from fine tuning transfer learning. But as of this writing, the two approaches are used hand-in-hand for natural language processing models.

Transformers are extremely accurate on pre-defined natural language processing tasks, but they are not without problems. First, and most famously, these models are huge and expensive. BERT has 110 million parameters, XLM-R has 550 million parameters, and GPT-3 has a whopping 175 billion parameters.[^biggpt] These base models can cost from hundreds to tens-of-thousands of dollars in compute costs for a single run [@hao2020mit]. GPT-3 is rumored to have cost $4.6 million in compute costs [@chuan2020lambda]. These figures exclude trial and error runs inherent in any model development process.

[^biggpt]: If each parameter is stored as a 4 byte float, then GPT-3 is 700 Gb on disk, larger than most data sets.

Second, the data sets used to build these base models affect results, but we don't know what the "right" data set looks like. It is not controversial to assert that models inherit biases from the data on which they are developed. Yet some of the issues in these large language models are particularly jarring. They can encode---then reproduce---racist or otherwise extreme language [@bender2021dangers]. And they may exclude language we wish was included, perhaps those of underrepresented groups. Yet the data sets used to construct transformers are so large, researchers cannot truly audit what they do or do not include. And the investment required to construct large language models comes at an opportunity cost of learning how to strategically construct data sets without these downsides [@bender2021dangers].[^samplingtheory]

[^samplingtheory]: To me, this sounds like a task that the statistics community is well suited for, adapting sampling theory and statistical design for use in constructing data sets of language.

Third, transformers are built for supervised tasks for artificial intelligence, not corpus analysis. Mostly these are sequence to sequence models used for question answering, machine translation, text generation, document summarization, and so on. While some of these tasks may be useful for corpus analysis, they really are distinct from a statistical analysis of a corpus. 

Fourth, and most obviously, transformers are deep neural networks. Deep neural networks are spectacular in their flexibility and accuracy for many supervised learning tasks. Yet this flexibility and accuracy has come at the cost of complexity, often antithetical to understanding. 

In summary, the fine-tuning approach of modeling would be beneficial to develop for Natural Language Statistics. It reflects how we intuitively understand that humans use language to learn themselves. Yet we would need simpler, more transparent models upon which to apply and build on statistical theory. Transformers are too big, complex, and opaque for this task. Boyd-Graber and Mimno make this point explicitly on p. 116 of _Applications of Topic Models_, "Deep learning has a reputation for inscrutable parameters but state-of-the-art performance. One of the strengths of probabilistic models is their interpretability and grounded generative processes" [@boydgraber2017applications].

## Summary of Research
I reexamine a model that has become less popular in machine learning circles, Latent Dirichlet Allocation (LDA) [@blei2002lda]. Despite this and with the above comments in mind, LDA has some desirable properties. It models a data generating process which may be linked to the empirical laws of language. This property makes LDA, and related models, candidates for helping to develop a more robust statistical theory for modeling language. And while LDA may be less popular at the cutting edge of machine learning, it and its variants are still popular in fields such as computational social science [@roberts2016textmodel] and the digital humanities [@erlin2018topic]. 

This dissertation makes four intellectual contributions, three studies and one software library. The first study investigates the data generating process modeled by LDA, linking it to empirical language laws and using it for simulation studies concerned with model specification. The second introduces a new (yet old) evaluation metric for topic models: a generalized coefficient of determination. The third study introduces a method for fine tuning transfer learning applicable to MCMC algorithms for LDA. Finally, this sort of research is not practical if people cannot use it. I have developed a software package for the R language [@rlang] that draws on this research and a framework known as the "tidyverse" [@tidyverse] to make a principled, flexible, performant, and user-friendly interface for training and using LDA models. This package is called _tidylda_.

